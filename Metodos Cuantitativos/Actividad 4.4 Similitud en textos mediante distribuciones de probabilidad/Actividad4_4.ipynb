{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selecciona un cuerpo de texto de interés (extensión .txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text from the specified file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_text(file_path, text):\n",
    "    \"\"\"\n",
    "    Saves the given text to a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file where the text will be saved.\n",
    "    text (str): The text to save.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Genera un cuerpo de texto sintético utilizando herramientas como MarkovifyLinks to an external site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_text(model, num_sentences=100):\n",
    "    \"\"\"\n",
    "    Generates synthetic text using a Markov model.\n",
    "\n",
    "    Parameters:\n",
    "    model (markovify.Text): The Markov model generated from the original text.\n",
    "    num_sentences (int): The number of sentences to generate.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated synthetic text as a single string.\n",
    "    \"\"\"\n",
    "    synthetic_text = \"\"\n",
    "    for _ in range(num_sentences):\n",
    "        sentence = model.make_sentence()\n",
    "        if sentence is not None:\n",
    "            synthetic_text += sentence + \" \"\n",
    "    return synthetic_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transforma el cuerpo de texto original y el sintético a una representación vectorial, por ejemplo tf–idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by converting to lowercase and removing stop words.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    list: The preprocessed words in the text.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "def compute_tf(word_dict, word_count):\n",
    "    \"\"\"\n",
    "    Computes the term frequency for each word in the word dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    word_dict (dict): A dictionary of words and their counts in the text.\n",
    "    word_count (int): The total number of words in the text.\n",
    "\n",
    "    Returns:\n",
    "    dict: The term frequency for each word.\n",
    "    \"\"\"\n",
    "    tf_dict = {}\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(word_count)\n",
    "    return tf_dict\n",
    "\n",
    "def compute_idf(documents):\n",
    "    \"\"\"\n",
    "    Computes the inverse document frequency for each word in the documents.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): A list of lists, where each sublist contains the words in a document.\n",
    "\n",
    "    Returns:\n",
    "    dict: The inverse document frequency for each word.\n",
    "    \"\"\"\n",
    "    N = len(documents)\n",
    "    idf_dict = dict.fromkeys(documents[0], 0)\n",
    "    for document in documents:\n",
    "        for word in document:\n",
    "            if word in idf_dict:\n",
    "                idf_dict[word] += 1\n",
    "\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "    return idf_dict\n",
    "\n",
    "def compute_tfidf(tf, idf):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF score for each word.\n",
    "\n",
    "    Parameters:\n",
    "    tf (dict): The term frequency for each word.\n",
    "    idf (dict): The inverse document frequency for each word.\n",
    "\n",
    "    Returns:\n",
    "    dict: The TF-IDF score for each word.\n",
    "    \"\"\"\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val * idf[word]\n",
    "    return tfidf\n",
    "\n",
    "def transform_to_tfidf_vector(text, idf_dict):\n",
    "    \"\"\"\n",
    "    Transforms the given text into a TF-IDF vector representation.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to transform.\n",
    "    idf_dict (dict): The inverse document frequency for each word.\n",
    "\n",
    "    Returns:\n",
    "    dict: The TF-IDF vector representation of the text.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    word_count = len(words)\n",
    "    word_dict = Counter(words)\n",
    "    tf = compute_tf(word_dict, word_count)\n",
    "    tfidf = compute_tfidf(tf, idf_dict)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utiliza una métrica de similitud como la distancia del coseno para obtener un valor de similitud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(tfidf1, tfidf2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two TF-IDF vectors.\n",
    "\n",
    "    Parameters:\n",
    "    tfidf1 (dict): The TF-IDF vector representation of the first text.\n",
    "    tfidf2 (dict): The TF-IDF vector representation of the second text.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity score between the two texts.\n",
    "    \"\"\"\n",
    "    common_words = set(tfidf1.keys()).intersection(set(tfidf2.keys()))\n",
    "    dot_product = sum(tfidf1[word] * tfidf2[word] for word in common_words)\n",
    "    \n",
    "    magnitude1 = math.sqrt(sum([val ** 2 for val in tfidf1.values()]))\n",
    "    magnitude2 = math.sqrt(sum([val ** 2 for val in tfidf2.values()]))\n",
    "    \n",
    "    if not magnitude1 or not magnitude2:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF vector:\n",
      " {'kaladin': -0.18380427608846076, '(apodado': 0.0025297342356202382, 'kal)': 0.0025297342356202382, 'es': 0.0, 'niño': 0.0025297342356202382, 'que': -0.15973197438417586, 'vive': 0.0025297342356202382, 'en': -0.032696340679344064, 'pueblo': -0.010978667129753825, 'llamado': 0.0025297342356202382, 'piedralar': 0.0025297342356202382, 'su': -0.09930503304123081, 'madre': 0.0025297342356202382, 'hesina,': 0.0025297342356202382, 'hermano': -0.028496602395376467, 'tien': -0.01872225808366085, 'y': -0.14156921814152473, 'padre': -0.05598636742944556, 'lirin.': 0.0025297342356202382, 'cirujano': 0.0025297342356202382, 'experto': 0.0025297342356202382, 'cura': 0.0025297342356202382, 'las': -0.02744666782438456, 'personas': 0.0025297342356202382, 'heridas': 0.0025297342356202382, 'junto': 0.0025297342356202382, 'kaladin,': -0.01872225808366085, 'ya': 0.0, 'soporta': -0.009217987752949838, 'ver': -0.009217987752949838, 'la': -0.10438862946398363, 'sangre.': -0.009217987752949838, 'día': -0.01872225808366085, 'laral,': 0.0025297342356202382, 'una': -0.050015328903744535, 'amiga': 0.0025297342356202382, 'suya,': 0.0025297342356202382, 'se': -0.0921751947781055, 'encontraban': 0.0025297342356202382, 'jugando': 0.0025297342356202382, 'entre': 0.0, 'rocas.': 0.0025297342356202382, 'laral': 0.0025297342356202382, 'propuso': 0.0025297342356202382, 'ir': 0.0025297342356202382, 'hablar': 0.0025297342356202382, 'los': -0.010978667129753825, 'otros': 0.0025297342356202382, 'chicos,': 0.0025297342356202382, 'pero': 0.0, 'hubo': 0.0025297342356202382, 'discursión': 0.0025297342356202382, 'jost,': -0.0050594684712404765, 'uno': -0.0050594684712404765, 'chicos.': -0.0050594684712404765, 'jost': -0.010978667129753825, 'retó': -0.0050594684712404765, 'batalla': -0.005873860994285037, 'palos.': -0.005873860994285037, 'al': 0.0025297342356202382, 'coger': 0.0025297342356202382, 'el': -0.046089938764749185, 'palo': 0.0025297342356202382, 'sintió': 0.0025297342356202382, 'buena': 0.0025297342356202382, 'sensación,': 0.0025297342356202382, 'después': 0.0, 'del': 0.0025297342356202382, 'combate': 0.0025297342356202382, 'pide': 0.0025297342356202382, 'le': -0.04219520814446956, 'enseñe': 0.0025297342356202382, 'combatir,': 0.0025297342356202382, 'este': -0.022060981976740127, 'quiere.': 0.0025297342356202382, 'todo': -0.004009533900248575, 'marchaba': -0.004009533900248575, 'bien': -0.004009533900248575, 'para': -0.004009533900248575, 'familia': -0.029414642635653496, 'hasta': -0.0025297342356202382, 'brillante': 0.0025297342356202382, 'señor': 0.0025297342356202382, 'wistiow,': 0.0025297342356202382, 'murió': 0.0025297342356202382, 'manos': 0.0025297342356202382, 'linir': 0.0025297342356202382, 'cuando': -0.02840744743146443, 'intentaba': 0.0025297342356202382, 'salvarle': 0.0025297342356202382, 'vida.': 0.0025297342356202382, 'unos': 0.0025297342356202382, 'días': 0.0025297342356202382, 'wistiow': 0.0025297342356202382, 'fue': 0.0, 'sustituido': 0.0025297342356202382, 'por': 0.0025297342356202382, 'roshone,': 0.0025297342356202382, 'ojos': 0.0025297342356202382, 'claros': 0.0025297342356202382, 'arrogante': 0.0025297342356202382, 'odia': 0.0025297342356202382, 'kaladin.': 0.0025297342356202382, 'rumorea': -0.0050594684712404765, 'robó': 0.0025297342356202382, 'esferas': 0.0, 'wistow,': 0.0025297342356202382, 'verdad': 0.0025297342356202382, 'lindir': 0.0, 'wistow': 0.0, 'firmaron': 0.0025297342356202382, 'tratado': 0.0025297342356202382, 'si': -0.013716382866737608, 'moría,': 0.0025297342356202382, 'quedaba': 0.0025297342356202382, 'esferas.': -0.00801906780049715, 'ha': 0.0025297342356202382, 'decidido': 0.0025297342356202382, 'quiere': 0.0025297342356202382, 'ser': 0.0025297342356202382, 'guerrero.': 0.0025297342356202382, 'acompaña': 0.0025297342356202382, 'charla': -0.00801906780049715, 'roshone.': -0.00801906780049715, 'enfada': 0.0025297342356202382, 'roshone': -0.018288510488983476, 'cena,': 0.0025297342356202382, 'dice': -0.03102058405900967, 'dejará': 0.0025297342356202382, 'empaz': 0.0025297342356202382, 'dan': -0.003344126758664799, 'más': -0.003344126758664799, 'mitad': -0.003344126758664799, 'lirin': -0.0025297342356202382, 'vaya,': -0.006831394806210187, 'lo': -0.006831394806210187, 'hace.': -0.006831394806210187, 'marcha': -0.0164327868511423, 'encuentra': -0.0025297342356202382, 'rillir,': -0.0025297342356202382, 'hijo': -0.0025297342356202382, 'laral.': -0.0164327868511423, 'rillir': -0.004572127622245869, 'empieza': -0.004572127622245869, 'insultar': -0.004572127622245869, 'vuelve': -0.009759666603746456, 'recogerle': -0.009759666603746456, 'él': 0.0025297342356202382, 'han': 0.0025297342356202382, 'llegado': 0.0025297342356202382, 'ningún': 0.0025297342356202382, 'acuerdo': 0.0025297342356202382, 'también': 0.0025297342356202382, 'cuenta': 0.0025297342356202382, 'realidad': 0.0025297342356202382, 'están': 0.0025297342356202382, 'robadas.': 0.0025297342356202382}\n",
      "Synthetic TF-IDF vector:\n",
      " {'rillir': -0.008021961377771834, 'empieza': -0.008021961377771834, 'insultar': -0.008021961377771834, 'la': -0.13227758738908205, 'familia': -0.023654144317998838, 'kaladin': -0.22651165781720203, 'y': -0.21451712903214643, 'jost,': -0.010356521374428211, 'uno': -0.010356521374428211, 'los': -0.011236437329171739, 'chicos.': -0.010356521374428211, 'cuando': -0.020767450897068447, 'se': -0.13140129821222457, 'marcha': -0.04084520872605167, 'laral.': -0.04084520872605167, 'día': -0.06569774234693371, 'kaladin,': -0.06569774234693371, 'su': -0.18875375010291578, 'hermano': -0.06666441029953597, 'tien': -0.06569774234693371, 'soporta': -0.06469315631099054, 'ver': -0.06469315631099054, 'sangre.': -0.06469315631099054, 'padre': -0.12442477218257998, 'vuelve': -0.07991052527635305, 'recogerle': -0.07991052527635305, 'le': -0.09871066835931407, 'dice': -0.09373491319716404, 'una': -0.07605319169909842, 'batalla': -0.01545884867866265, 'palos.': -0.01545884867866265, 'en': -0.013385609695407027, 'el': -0.05391096359249212, 'pueblo': -0.011236437329171739, 'rumorea': -0.010356521374428211, 'que': -0.06850689018767785, 'si': -0.00534797425184789, 'dan': -0.003911593305759467, 'más': -0.003911593305759467, 'mitad': -0.003911593305759467, 'las': -0.00642082133095528, 'esferas.': -0.004689913718967384, 'vaya,': -0.023971852852528383, 'este': -0.02580452107418055, 'lo': -0.023971852852528383, 'hace.': -0.023971852852528383, 'charla': -0.03986426661122276, 'roshone.': -0.03986426661122276, 'lirin': -0.002219254580234617, 'jost': -0.011236437329171739, 'retó': -0.010356521374428211, 'todo': -0.005862392148709231, 'marchaba': -0.005862392148709231, 'bien': -0.005862392148709231, 'para': -0.005862392148709231, 'encuentra': -0.002219254580234617, 'rillir,': -0.002219254580234617, 'hijo': -0.002219254580234617, 'roshone': -0.004010980688885917, 'hasta': -0.002219254580234617}\n",
      "Cosine similarity: 0.8772453481006331\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_TEXT = read_text(\"texto.txt\")\n",
    "SYNTETIC_TEXT = \"texto_sintetico.txt\"\n",
    "\n",
    "markov_model = markovify.Text(ORIGINAL_TEXT) # Build the Markov model\n",
    "synthetic_text = generate_synthetic_text(markov_model, num_sentences=100)\n",
    "\n",
    "save_text(SYNTETIC_TEXT, synthetic_text)\n",
    "\n",
    "#Problem 3\n",
    "original_words = preprocess_text(ORIGINAL_TEXT)\n",
    "synthetic_words = preprocess_text(synthetic_text)\n",
    "idf_dict = compute_idf([original_words, synthetic_words])\n",
    "\n",
    "original_tfidf_vector = transform_to_tfidf_vector(ORIGINAL_TEXT, idf_dict)\n",
    "synthetic_tfidf_vector = transform_to_tfidf_vector(synthetic_text, idf_dict)\n",
    "\n",
    "print(\"Original TF-IDF vector:\\n\", original_tfidf_vector)\n",
    "print(\"Synthetic TF-IDF vector:\\n\", synthetic_tfidf_vector)\n",
    "\n",
    "#Problem 4\n",
    "similarity_score = calculate_cosine_similarity(original_tfidf_vector, synthetic_tfidf_vector)\n",
    "print(\"Cosine similarity:\", similarity_score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
