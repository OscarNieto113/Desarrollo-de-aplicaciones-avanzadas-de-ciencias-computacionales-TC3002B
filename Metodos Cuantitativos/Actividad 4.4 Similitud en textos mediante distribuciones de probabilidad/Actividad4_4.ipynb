{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import tokenize\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selecciona un cuerpo de texto de interés (extensión .txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text from the specified file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_text(file_path, text):\n",
    "    \"\"\"\n",
    "    Saves the given text to a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file where the text will be saved.\n",
    "    text (str): The text to save.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Genera un cuerpo de texto sintético utilizando herramientas como MarkovifyLinks to an external site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_text(model, num_sentences=100):\n",
    "    \"\"\"\n",
    "    Generates synthetic text using a Markov model.\n",
    "\n",
    "    Parameters:\n",
    "    model (markovify.Text): The Markov model generated from the original text.\n",
    "    num_sentences (int): The number of sentences to generate.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated synthetic text as a single string.\n",
    "    \"\"\"\n",
    "    synthetic_text = \"\"\n",
    "    for _ in range(num_sentences):\n",
    "        sentence = model.make_sentence()\n",
    "        if sentence is not None:\n",
    "            synthetic_text += sentence + \" \"\n",
    "    return synthetic_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transforma el cuerpo de texto original y el sintético a una representación vectorial, por ejemplo tf–idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by converting to lowercase and removing stop words.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    list: The preprocessed words in the text.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "def compute_tf(word_dict, word_count):\n",
    "    \"\"\"\n",
    "    Computes the term frequency for each word in the word dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    word_dict (dict): A dictionary of words and their counts in the text.\n",
    "    word_count (int): The total number of words in the text.\n",
    "\n",
    "    Returns:\n",
    "    dict: The term frequency for each word.\n",
    "    \"\"\"\n",
    "    tf_dict = {}\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(word_count)\n",
    "    return tf_dict\n",
    "\n",
    "def compute_idf(documents):\n",
    "    \"\"\"\n",
    "    Computes the inverse document frequency for each word in the documents.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): A list of lists, where each sublist contains the words in a document.\n",
    "\n",
    "    Returns:\n",
    "    dict: The inverse document frequency for each word.\n",
    "    \"\"\"\n",
    "    N = len(documents)\n",
    "    idf_dict = dict.fromkeys(documents[0], 0)\n",
    "    for document in documents:\n",
    "        for word in document:\n",
    "            if word in idf_dict:\n",
    "                idf_dict[word] += 1\n",
    "\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "    return idf_dict\n",
    "\n",
    "def compute_tfidf(tf, idf):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF score for each term in a document.\n",
    "\n",
    "    Parameters:\n",
    "    tf (dict): A dictionary mapping terms to their TF values.\n",
    "    idf (dict): A dictionary mapping terms to their IDF values.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping terms to their TF-IDF values.\n",
    "    \"\"\"\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val * idf.get(word, 0.0)\n",
    "    return tfidf\n",
    "\n",
    "def transform_to_tfidf_vector(text, idf_dict):\n",
    "    \"\"\"\n",
    "    Transforms the given text into a TF-IDF vector representation.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to transform.\n",
    "    idf_dict (dict): The inverse document frequency for each word.\n",
    "\n",
    "    Returns:\n",
    "    dict: The TF-IDF vector representation of the text.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    word_count = len(words)\n",
    "    word_dict = Counter(words)\n",
    "    tf = compute_tf(word_dict, word_count)\n",
    "    tfidf = compute_tfidf(tf, idf_dict)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utiliza una métrica de similitud como la distancia del coseno para obtener un valor de similitud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(tfidf1, tfidf2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two TF-IDF vectors.\n",
    "\n",
    "    Parameters:\n",
    "    tfidf1 (dict): The TF-IDF vector representation of the first text.\n",
    "    tfidf2 (dict): The TF-IDF vector representation of the second text.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity score between the two texts.\n",
    "    \"\"\"\n",
    "    common_words = set(tfidf1.keys()).intersection(set(tfidf2.keys()))\n",
    "\n",
    "    dot_product = 0\n",
    "    for word in common_words:\n",
    "        dot_product += tfidf1[word] * tfidf2[word]\n",
    "    \n",
    "    magnitude1 = 0\n",
    "    for val in tfidf1.values():\n",
    "        magnitude1 += val ** 2\n",
    "    magnitude1 = math.sqrt(magnitude1)\n",
    "    \n",
    "    magnitude2 = 0\n",
    "    for val in tfidf2.values():\n",
    "        magnitude2 += val ** 2\n",
    "    magnitude2 = math.sqrt(magnitude2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (magnitude1 * magnitude2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compara contra otros cuerpos de texto.(CODIGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_code(code):\n",
    "    \"\"\"\n",
    "    Preprocesses the code by tokenizing it.\n",
    "\n",
    "    Parameters:\n",
    "    code (str): The code to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    list: The tokens in the code.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    try:\n",
    "        bytes_io = BytesIO(code.encode('utf-8'))\n",
    "        for token in tokenize.tokenize(bytes_io.readline):\n",
    "            # Exclude specific token types that are not useful for analysis\n",
    "            if token.type not in [\n",
    "                tokenize.ENCODING, \n",
    "                tokenize.ENDMARKER, \n",
    "                tokenize.NEWLINE, \n",
    "                tokenize.NL, \n",
    "                tokenize.INDENT, \n",
    "                tokenize.DEDENT,\n",
    "                tokenize.COMMENT\n",
    "            ]:\n",
    "                tokens.append(token.string.lower())\n",
    "    except tokenize.TokenError:\n",
    "        pass\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF vector:\n",
      " {'kaladin': -0.18784018031641994, '(apodado': 0.0025297342356202382, 'kal)': 0.0025297342356202382, 'es': 0.0, 'niño': 0.0025297342356202382, 'que': -0.17044791838742926, 'vive': 0.0025297342356202382, 'en': -0.035509309289330535, 'pueblo': -0.012443416731667337, 'llamado': 0.0025297342356202382, 'piedralar': 0.0025297342356202382, 'su': -0.09278989285687432, 'madre': 0.0025297342356202382, 'hesina,': 0.0025297342356202382, 'hermano': -0.022767608120582143, 'tien': -0.014707321317826748, 'y': -0.1427329513736626, 'padre': -0.057609496736315935, 'lirin.': 0.0025297342356202382, 'cirujano': 0.0025297342356202382, 'experto': 0.0025297342356202382, 'cura': 0.0025297342356202382, 'las': -0.03676830329456687, 'personas': 0.0025297342356202382, 'heridas': 0.0025297342356202382, 'junto': 0.0025297342356202382, 'kaladin,': -0.014707321317826748, 'ya': 0.0, 'soporta': -0.007101861857866107, 'ver': -0.007101861857866107, 'la': -0.10506590532100779, 'sangre.': -0.007101861857866107, 'día': -0.014707321317826748, 'laral,': 0.0025297342356202382, 'una': -0.04456837655783219, 'amiga': 0.0025297342356202382, 'suya,': 0.0025297342356202382, 'se': -0.09568305789758179, 'encontraban': 0.0025297342356202382, 'jugando': 0.0025297342356202382, 'entre': 0.0, 'rocas.': 0.0025297342356202382, 'laral': 0.0025297342356202382, 'propuso': 0.0025297342356202382, 'ir': 0.0025297342356202382, 'hablar': 0.0025297342356202382, 'los': -0.014707321317826748, 'otros': 0.0025297342356202382, 'chicos,': 0.0025297342356202382, 'pero': 0.0, 'hubo': 0.0025297342356202382, 'discursión': 0.0025297342356202382, 'jost,': -0.007101861857866107, 'uno': -0.007101861857866107, 'chicos.': -0.007101861857866107, 'jost': -0.006688253517329598, 'retó': -0.0025297342356202382, 'batalla': -0.004009533900248575, 'palos.': -0.004009533900248575, 'al': 0.0025297342356202382, 'coger': 0.0025297342356202382, 'el': -0.04941697447266806, 'palo': 0.0025297342356202382, 'sintió': 0.0025297342356202382, 'buena': 0.0025297342356202382, 'sensación,': 0.0025297342356202382, 'después': 0.0, 'del': 0.0025297342356202382, 'combate': 0.0025297342356202382, 'pide': 0.0025297342356202382, 'le': -0.04409379395831186, 'enseñe': 0.0025297342356202382, 'combatir,': 0.0025297342356202382, 'este': -0.024057203401491457, 'quiere.': 0.0025297342356202382, 'todo': -0.0054893335648769125, 'marchaba': -0.0054893335648769125, 'bien': -0.0054893335648769125, 'para': -0.0054893335648769125, 'familia': -0.03432664608997778, 'hasta': -0.004572127622245869, 'brillante': 0.0025297342356202382, 'señor': 0.0025297342356202382, 'wistiow,': 0.0025297342356202382, 'murió': 0.0025297342356202382, 'manos': 0.0025297342356202382, 'linir': 0.0025297342356202382, 'cuando': -0.03432664608997778, 'intentaba': 0.0025297342356202382, 'salvarle': 0.0025297342356202382, 'vida.': 0.0025297342356202382, 'unos': 0.0025297342356202382, 'días': 0.0025297342356202382, 'wistiow': 0.0025297342356202382, 'fue': 0.0, 'sustituido': 0.0025297342356202382, 'por': 0.0025297342356202382, 'roshone,': 0.0025297342356202382, 'ojos': 0.0025297342356202382, 'claros': 0.0025297342356202382, 'arrogante': 0.0025297342356202382, 'odia': 0.0025297342356202382, 'kaladin.': 0.0025297342356202382, 'rumorea': -0.005873860994285037, 'robó': 0.0025297342356202382, 'esferas': 0.0, 'wistow,': 0.0025297342356202382, 'verdad': 0.0025297342356202382, 'lindir': 0.0, 'wistow': 0.0, 'firmaron': 0.0025297342356202382, 'tratado': 0.0025297342356202382, 'si': -0.020494184418630563, 'moría,': 0.0025297342356202382, 'quedaba': 0.0025297342356202382, 'esferas.': -0.013078536271737627, 'ha': 0.0025297342356202382, 'decidido': 0.0025297342356202382, 'quiere': 0.0025297342356202382, 'ser': 0.0025297342356202382, 'guerrero.': 0.0025297342356202382, 'acompaña': 0.0025297342356202382, 'charla': -0.007101861857866107, 'roshone.': -0.007101861857866107, 'enfada': 0.0025297342356202382, 'roshone': -0.013376507034659197, 'cena,': 0.0025297342356202382, 'dice': -0.030693726798246734, 'dejará': 0.0025297342356202382, 'empaz': 0.0025297342356202382, 'dan': -0.006221708365833668, 'más': -0.006221708365833668, 'mitad': -0.006221708365833668, 'lirin': -0.0025297342356202382, 'vaya,': -0.007589202706860714, 'lo': -0.007589202706860714, 'hace.': -0.007589202706860714, 'marcha': -0.017502885202907815, 'encuentra': 0.0, 'rillir,': 0.0, 'hijo': 0.0, 'laral.': -0.017502885202907815, 'rillir': -0.005873860994285037, 'empieza': -0.005873860994285037, 'insultar': -0.005873860994285037, 'vuelve': -0.009631596093486346, 'recogerle': -0.009631596093486346, 'él': 0.0025297342356202382, 'han': 0.0025297342356202382, 'llegado': 0.0025297342356202382, 'ningún': 0.0025297342356202382, 'acuerdo': 0.0025297342356202382, 'también': 0.0025297342356202382, 'cuenta': 0.0025297342356202382, 'realidad': 0.0025297342356202382, 'están': 0.0025297342356202382, 'robadas.': 0.0025297342356202382}\n",
      "Synthetic TF-IDF vector:\n",
      " {'lirin': -0.002159337011090172, 'dice': -0.08733209909366152, 'kaladin': -0.24814075147867076, 'una': -0.04565134645549914, 'charla': -0.026268776674682318, 'roshone.': -0.026268776674682318, 'rillir': -0.015041475817141122, 'empieza': -0.015041475817141122, 'insultar': -0.015041475817141122, 'la': -0.1328628501578684, 'familia': -0.041509220531442494, 'y': -0.21782506295729534, 'jost,': -0.026268776674682318, 'uno': -0.026268776674682318, 'los': -0.02720014461791219, 'chicos.': -0.026268776674682318, 'jost': -0.0028544882612902022, 'retó': -0.002159337011090172, 'su': -0.1320063896302366, 'padre': -0.13768849188816693, 'vuelve': -0.07399226157799792, 'recogerle': -0.07399226157799792, 'le': -0.11604955429630469, 'que': -0.09699424639284655, 'se': -0.1531376043454288, 'vaya,': -0.03239005516635258, 'este': -0.03422468188997227, 'lo': -0.03239005516635258, 'hace.': -0.03239005516635258, 'todo': -0.012494931645078082, 'marchaba': -0.012494931645078082, 'bien': -0.012494931645078082, 'para': -0.012494931645078082, 'en': -0.01818607615939545, 'el': -0.07030244551148002, 'pueblo': -0.01593222516110678, 'rumorea': -0.015041475817141122, 'si': -0.019437198098666578, 'dan': -0.017702472401229755, 'más': -0.017702472401229755, 'mitad': -0.017702472401229755, 'las': -0.020923188167624764, 'esferas.': -0.01860601733362466, 'día': -0.02720014461791219, 'kaladin,': -0.02720014461791219, 'hermano': -0.028071381144172236, 'tien': -0.02720014461791219, 'soporta': -0.026268776674682318, 'ver': -0.026268776674682318, 'sangre.': -0.026268776674682318, 'cuando': -0.041509220531442494, 'marcha': -0.04980052487639399, 'laral.': -0.04980052487639399, 'batalla': -0.005704113648328711, 'palos.': -0.005704113648328711, 'hasta': -0.007805376750749956, 'encuentra': 0.0, 'rillir,': 0.0, 'hijo': 0.0, 'roshone': -0.0009514960870967341}\n",
      "Cosine similarity: 0.9122298660300862\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_TEXT = read_text(\"texto.txt\")\n",
    "SYNTETIC_TEXT = \"texto_sintetico.txt\"\n",
    "\n",
    "#Problem 2\n",
    "markov_model = markovify.Text(ORIGINAL_TEXT) # Build the Markov model\n",
    "synthetic_text = generate_synthetic_text(markov_model, num_sentences=100)\n",
    "\n",
    "save_text(SYNTETIC_TEXT, synthetic_text)\n",
    "\n",
    "#Problem 3\n",
    "original_words = preprocess_text(ORIGINAL_TEXT)\n",
    "synthetic_words = preprocess_text(synthetic_text)\n",
    "idf_dict = compute_idf([original_words, synthetic_words])\n",
    "\n",
    "original_tfidf_vector = transform_to_tfidf_vector(ORIGINAL_TEXT, idf_dict)\n",
    "synthetic_tfidf_vector = transform_to_tfidf_vector(synthetic_text, idf_dict)\n",
    "\n",
    "print(\"Original TF-IDF vector:\\n\", original_tfidf_vector)\n",
    "print(\"Synthetic TF-IDF vector:\\n\", synthetic_tfidf_vector)\n",
    "\n",
    "#Problem 4\n",
    "similarity_score = calculate_cosine_similarity(original_tfidf_vector, synthetic_tfidf_vector)\n",
    "print(\"Cosine similarity:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF vector:\n",
      " {'//': -0.06120870090863479, 'c': 0.0, 'program': 0.0033085784274937725, \"dijkstra's\": 0.0, 'single': 0.0, 'source': 0.0, 'shortest': 0.0, 'path': 0.0, 'algorithm.': 0.0, 'adjacency': 0.0, 'matrix': 0.0, 'representation': 0.0, 'graph': 0.0, '#include': 0.0, '<limits.h>': 0.0, '<stdbool.h>': 0.0, '<stdio.h>': 0.0, 'number': 0.0, 'vertices': 0.0, '#define': 0.0, 'v': 0.0, '9': 0.0, 'utility': 0.0, 'function': 0.0, 'vertex': 0.0, 'minimum': 0.0, 'distance': 0.0, 'value,': 0.0, 'set': 0.0, 'included': 0.0, 'tree': 0.0, 'int': 0.0, 'mindistance(int': 0.0, 'dist[],': 0.0, 'bool': 0.0, 'sptset[])': 0.0, '{': 0.0, 'initialize': 0.0, 'min': 0.0, 'value': 0.0, '=': 0.0, 'int_max,': 0.0, 'min_index;': 0.0, '(int': 0.0, '0;': 0.0, '<': 0.0, 'v;': 0.0, 'v++)': 0.0, '(sptset[v]': 0.0, '==': 0.0, 'false': 0.0, '&&': 0.0, 'dist[v]': 0.0, '<=': 0.0, 'min)': 0.0, 'dist[v],': 0.0, 'min_index': 0.0, 'return': 0.0, '}': 0.0, 'print': 0.0, 'constructed': 0.0, 'array': 0.0, 'void': 0.0, 'printsolution(int': 0.0, 'dist[])': 0.0, 'printf(\"vertex': 0.0, '\\\\t\\\\t': 0.0, 'source\\\\n\");': 0.0, 'i++)': 0.0, 'printf(\"%d': 0.0, '\\\\t\\\\t\\\\t\\\\t': 0.0, '%d\\\\n\",': 0.0, 'i,': 0.0, 'dist[i]);': 0.0, 'implements': 0.0, 'algorithm': 0.0, 'represented': 0.0, 'using': 0.0, 'dijkstra(int': 0.0, 'graph[v][v],': 0.0, 'src)': 0.0, 'dist[v];': 0.0, 'output': 0.0, 'array.': 0.0, 'dist[i]': 0.0, 'hold': 0.0, 'src': 0.0, 'sptset[v];': 0.0, 'sptset[i]': 0.0, 'true': 0.0, 'finalized': 0.0, 'distances': 0.0, 'infinite': 0.0, 'stpset[]': 0.0, 'false;': 0.0, '0': 0.0, 'dist[src]': 0.0, 'count': 0.0, '-': 0.0, '1;': 0.0, 'count++)': 0.0, 'pick': 0.0, 'processed.': 0.0, 'u': 0.0, 'equal': 0.0, 'iteration.': 0.0, 'mindistance(dist,': 0.0, 'sptset);': 0.0, 'mark': 0.0, 'picked': 0.0, 'processed': 0.0, 'sptset[u]': 0.0, 'true;': 0.0, 'update': 0.0, 'dist': 0.0, 'adjacent': 0.0, 'vertex.': 0.0, 'sptset,': 0.0, 'edge': 0.0, 'v,': 0.0, 'total': 0.0, 'weight': 0.0, 'smaller': 0.0, 'current': 0.0, '(!sptset[v]': 0.0, 'graph[u][v]': 0.0, 'dist[u]': 0.0, '!=': 0.0, 'int_max': 0.0, '+': 0.0, 'dist[v])': 0.0, 'graph[u][v];': 0.0, 'printsolution(dist);': 0.0, \"driver's\": 0.0, 'code': 0.0, 'main()': 0.0, '/*': 0.0, 'let': 0.0, 'create': 0.0, 'example': 0.0, 'discussed': 0.0, '*/': 0.0, 'graph[v][v]': 0.0, '0,': 0.0, '4,': 0.0, '8,': 0.0, '},': 0.0, '11,': 0.0, '7,': 0.0, '2': 0.0, '9,': 0.0, '14,': 0.0, '10,': 0.0, '2,': 0.0, '1,': 0.0, '6': 0.0, '7': 0.0, '6,': 0.0, '};': 0.0, 'dijkstra(graph,': 0.0, '0);': 0.0}\n",
      "Synthetic TF-IDF vector:\n",
      " {'/*': 0.0, 'copyright': 0.0, '(c)': 0.0, '2012,': 0.0, 'sean': 0.0, 'heber.': 0.0, 'rights': 0.0, 'reserved.': 0.0, 'redistribution': 0.0, 'use': 0.0, 'source': 0.0, 'binary': 0.0, 'forms,': 0.0, 'modification,': 0.0, 'permitted': 0.0, 'provided': 0.0, 'following': 0.0, 'conditions': 0.0, 'met:': 0.0, '1.': 0.0, 'redistributions': 0.0, 'code': 0.0, 'retain': 0.0, 'notice,': 0.0, 'list': 0.0, 'disclaimer.': 0.0, '2.': 0.0, 'form': 0.0, 'reproduce': 0.0, 'disclaimer': 0.0, 'documentation': 0.0, 'and/or': 0.0, 'materials': 0.0, 'distribution.': 0.0, '3.': 0.0, 'heber': 0.0, 'names': 0.0, 'contributors': 0.0, 'used': 0.0, 'endorse': 0.0, 'promote': 0.0, 'products': 0.0, 'derived': 0.0, 'software': 0.0, 'specific': 0.0, 'prior': 0.0, 'written': 0.0, 'permission.': 0.0, 'holders': 0.0, '\"as': 0.0, 'is\"': 0.0, 'express': 0.0, 'implied': 0.0, 'warranties,': 0.0, 'including,': 0.0, 'limited': 0.0, 'to,': 0.0, 'warranties': 0.0, 'merchantability': 0.0, 'fitness': 0.0, 'particular': 0.0, 'purpose': 0.0, 'disclaimed.': 0.0, 'event': 0.0, 'shall': 0.0, 'liable': 0.0, 'direct,': 0.0, 'indirect,': 0.0, 'incidental,': 0.0, 'special,': 0.0, 'exemplary,': 0.0, 'consequential': 0.0, 'damages': 0.0, '(including,': 0.0, 'procurement': 0.0, 'substitute': 0.0, 'goods': 0.0, 'services;': 0.0, 'loss': 0.0, 'use,': 0.0, 'data,': 0.0, 'profits;': 0.0, 'business': 0.0, 'interruption)': 0.0, 'caused': 0.0, 'theory': 0.0, 'liability,': 0.0, 'contract,': 0.0, 'strict': 0.0, 'tort': 0.0, '(including': 0.0, 'negligence': 0.0, 'otherwise)': 0.0, 'arising': 0.0, 'way': 0.0, 'software,': 0.0, 'advised': 0.0, 'possibility': 0.0, 'damage.': 0.0, '*/': 0.0, '#include': 0.0, '\"astar.h\"': 0.0, '<math.h>': 0.0, '<string.h>': 0.0, '<stdint.h>': 0.0, 'struct': 0.0, '__asneighborlist': 0.0, '{': 0.0, 'const': 0.0, 'aspathnodesource': 0.0, '*source;': 0.0, 'size_t': 0.0, 'capacity;': 0.0, 'count;': 0.0, 'float': 0.0, '*costs;': 0.0, 'void': 0.0, '*nodekeys;': 0.0, '};': 0.0, '__aspath': 0.0, 'nodesize;': 0.0, 'cost;': 0.0, 'int8_t': 0.0, 'nodekeys[];': 0.0, 'typedef': 0.0, 'unsigned': 0.0, 'isclosed:1;': 0.0, 'isopen:1;': 0.0, 'isgoal:1;': 0.0, 'hasparent:1;': 0.0, 'hasestimatedcost:1;': 0.0, 'estimatedcost;': 0.0, 'openindex;': 0.0, 'parentindex;': 0.0, 'nodekey[];': 0.0, '}': 0.0, 'noderecord;': 0.0, '__visitednodes': 0.0, '*context;': 0.0, 'noderecordscapacity;': 0.0, 'noderecordscount;': 0.0, '*noderecords;': 0.0, '*noderecordsindex;': 0.0, '//': -0.0030006371452811482, 'array': 0.0, 'noderecords': 0.0, 'indexes,': 0.0, 'kept': 0.0, 'sorted': 0.0, 'noderecords[i]->nodekey': 0.0, 'using': 0.0, 'source->nodecomparator': 0.0, 'opennodescapacity;': 0.0, 'opennodescount;': 0.0, '*opennodes;': 0.0, 'heap': 0.0, 'noderecords[i]->rank': 0.0, '*visitednodes;': 0.0, 'visitednodes': 0.0, 'nodes;': 0.0, 'index;': 0.0, 'node;': 0.0, 'static': 0.0, 'node': 0.0, 'nodenull': 0.0, '=': 0.0, '{null,': 0.0, '-1};': 0.0, '/********************************************/': 0.0, 'inline': 0.0, 'visitednodescreate(const': 0.0, '*source,': 0.0, '*context)': 0.0, 'nodes': 0.0, 'calloc(1,': 0.0, 'sizeof(struct': 0.0, '__visitednodes));': 0.0, 'nodes->source': 0.0, 'source;': 0.0, 'nodes->context': 0.0, 'context;': 0.0, 'return': 0.0, 'visitednodesdestroy(visitednodes': 0.0, 'visitednodes)': 0.0, 'free(visitednodes->noderecordsindex);': 0.0, 'free(visitednodes->noderecords);': 0.0, 'free(visitednodes->opennodes);': 0.0, 'free(visitednodes);': 0.0, 'int': 0.0, 'nodeisnull(node': 0.0, 'n)': 0.0, '(n.nodes': 0.0, '==': 0.0, 'nodenull.nodes)': 0.0, '&&': 0.0, '(n.index': 0.0, 'nodenull.index);': 0.0, 'nodemake(visitednodes': 0.0, 'nodes,': 0.0, 'index)': 0.0, '(node){nodes,': 0.0, 'index};': 0.0, 'noderecord': 0.0, '*nodegetrecord(node': 0.0, 'node)': 0.0, 'node.nodes->noderecords': 0.0, '+': 0.0, '(node.index': 0.0, '*': 0.0, '(node.nodes->source->nodesize': 0.0, 'sizeof(noderecord)));': 0.0, '*getnodekey(node': 0.0, 'nodegetrecord(node)->nodekey;': 0.0, 'nodeisinopenset(node': 0.0, 'nodegetrecord(n)->isopen;': 0.0, 'nodeisinclosedset(node': 0.0, 'nodegetrecord(n)->isclosed;': 0.0, 'removenodefromclosedset(node': 0.0, 'nodegetrecord(n)->isclosed': 0.0, '0;': 0.0, 'addnodetoclosedset(node': 0.0, '1;': 0.0, 'getnoderank(node': 0.0, '*record': 0.0, 'nodegetrecord(n);': 0.0, 'record->estimatedcost': 0.0, 'record->cost;': 0.0, 'getnodecost(node': 0.0, 'nodegetrecord(n)->cost;': 0.0, 'getnodeestimatedcost(node': 0.0, 'nodegetrecord(n)->estimatedcost;': 0.0, 'setnodeestimatedcost(node': 0.0, 'n,': 0.0, 'estimatedcost)': 0.0, 'record->hasestimatedcost': 0.0, 'nodehasestimatedcost(node': 0.0, 'nodegetrecord(n)->hasestimatedcost;': 0.0, 'setnodeisgoal(node': 0.0, '(!nodeisnull(n))': 0.0, 'nodegetrecord(n)->isgoal': 0.0, 'nodeisgoal(node': 0.0, '!nodeisnull(n)': 0.0, 'nodegetrecord(n)->isgoal;': 0.0, 'getparentnode(node': 0.0, '(record->hasparent)': 0.0, 'nodemake(n.nodes,': 0.0, 'record->parentindex);': 0.0, 'nodenull;': 0.0, 'noderankcompare(node': 0.0, 'n1,': 0.0, 'n2)': 0.0, 'rank1': 0.0, 'getnoderank(n1);': 0.0, 'rank2': 0.0, 'getnoderank(n2);': 0.0, '(rank1': 0.0, '<': 0.0, 'rank2)': 0.0, '-1;': 0.0, '>': 0.0, 'getpathcostheuristic(node': 0.0, 'a,': 0.0, 'b)': 0.0, '(a.nodes->source->pathcostheuristic': 0.0, '!nodeisnull(a)': 0.0, '!nodeisnull(b))': 0.0, 'a.nodes->source->pathcostheuristic(getnodekey(a),': 0.0, 'getnodekey(b),': 0.0, 'a.nodes->context);': 0.0, 'nodekeycompare(node': 0.0, 'node,': 0.0, '*nodekey)': 0.0, '(node.nodes->source->nodecomparator)': 0.0, 'node.nodes->source->nodecomparator(getnodekey(node),': 0.0, 'nodekey,': 0.0, 'node.nodes->context);': 0.0, 'memcmp(getnodekey(node),': 0.0, 'node.nodes->source->nodesize);': 0.0, 'getnode(visitednodes': 0.0, '(!nodekey)': 0.0, 'looks': 0.0, 'index,': 0.0, \"it's\": 0.0, 'inserts': 0.0, 'new': 0.0, 'record': 0.0, 'index': 0.0, 'returns': 0.0, 'reference': 0.0, '(nodes->noderecordscount': 0.0, '0)': 0.0, 'nodes->noderecordscount-1;': 0.0, '(first': 0.0, '<=': 0.0, 'last)': 0.0, 'mid': 0.0, '/': 0.0, '2;': 0.0, 'comp': 0.0, 'nodekeycompare(nodemake(nodes,': 0.0, 'nodes->noderecordsindex[mid]),': 0.0, 'nodekey);': 0.0, '(comp': 0.0, '0': 0.0, '-': 0.0, 'break;': 0.0, 'nodemake(nodes,': 0.0, 'nodes->noderecordsindex[mid]);': 0.0, 'nodes->noderecordscapacity)': 0.0, 'nodes->noderecordscapacity': 0.0, '1': 0.0, '(nodes->noderecordscapacity': 0.0, '2);': 0.0, 'nodes->noderecords': 0.0, 'realloc(nodes->noderecords,': 0.0, '(sizeof(noderecord)': 0.0, 'nodes->source->nodesize));': 0.0, 'nodes->noderecordsindex': 0.0, 'realloc(nodes->noderecordsindex,': 0.0, 'sizeof(size_t));': 0.0, 'nodes->noderecordscount);': 0.0, 'nodes->noderecordscount++;': 0.0, 'memmove(&nodes->noderecordsindex[first+1],': 0.0, '&nodes->noderecordsindex[first],': 0.0, '1)': 0.0, 'nodes->noderecordsindex[first]': 0.0, 'node.index;': 0.0, 'nodegetrecord(node);': 0.0, 'memset(record,': 0.0, '0,': 0.0, 'sizeof(noderecord));': 0.0, 'memcpy(record->nodekey,': 0.0, 'nodes->source->nodesize);': 0.0, 'swapopensetnodesatindexes(visitednodes': 0.0, 'index1,': 0.0, 'index2)': 0.0, '(index1': 0.0, '!=': 0.0, '*record1': 0.0, 'nodegetrecord(nodemake(nodes,': 0.0, 'nodes->opennodes[index1]));': 0.0, '*record2': 0.0, 'nodes->opennodes[index2]));': 0.0, 'tempopenindex': 0.0, 'record1->openindex;': 0.0, 'record1->openindex': 0.0, 'record2->openindex;': 0.0, 'record2->openindex': 0.0, 'tempopenindex;': 0.0, 'tempnodeindex': 0.0, 'nodes->opennodes[index1];': 0.0, 'nodes->opennodes[index1]': 0.0, 'nodes->opennodes[index2];': 0.0, 'nodes->opennodes[index2]': 0.0, 'tempnodeindex;': 0.0, 'didremovefromopensetatindex(visitednodes': 0.0, 'smallestindex': 0.0, '(smallestindex': 0.0, 'swapopensetnodesatindexes(nodes,': 0.0, 'smallestindex,': 0.0, 'index);': 0.0, 'smallestindex;': 0.0, 'leftindex': 0.0, '(2': 0.0, 'rightindex': 0.0, '(leftindex': 0.0, 'nodes->opennodescount': 0.0, 'noderankcompare(nodemake(nodes,': 0.0, 'nodes->opennodes[leftindex]),': 0.0, 'nodes->opennodes[smallestindex]))': 0.0, 'leftindex;': 0.0, '(rightindex': 0.0, 'nodes->opennodes[rightindex]),': 0.0, 'rightindex;': 0.0, 'removenodefromopenset(node': 0.0, '(record->isopen)': 0.0, 'record->isopen': 0.0, 'n.nodes->opennodescount--;': 0.0, 'record->openindex;': 0.0, 'swapopensetnodesatindexes(n.nodes,': 0.0, 'n.nodes->opennodescount);': 0.0, 'didremovefromopensetatindex(n.nodes,': 0.0, 'didinsertintoopensetatindex(visitednodes': 0.0, '(index': 0.0, 'parentindex': 0.0, 'floorf((index-1)': 0.0, '(noderankcompare(nodemake(nodes,': 0.0, 'nodes->opennodes[parentindex]),': 0.0, 'nodes->opennodes[index]))': 0.0, 'parentindex,': 0.0, 'addnodetoopenset(node': 0.0, 'cost,': 0.0, 'parent)': 0.0, '(!nodeisnull(parent))': 0.0, 'record->hasparent': 0.0, 'record->parentindex': 0.0, 'parent.index;': 0.0, '(n.nodes->opennodescount': 0.0, 'n.nodes->opennodescapacity)': 0.0, 'n.nodes->opennodescapacity': 0.0, '(n.nodes->opennodescapacity': 0.0, 'n.nodes->opennodes': 0.0, 'realloc(n.nodes->opennodes,': 0.0, 'openindex': 0.0, 'n.nodes->opennodescount;': 0.0, 'n.nodes->opennodes[openindex]': 0.0, 'n.index;': 0.0, 'n.nodes->opennodescount++;': 0.0, 'record->openindex': 0.0, 'record->cost': 0.0, 'didinsertintoopensetatindex(n.nodes,': 0.0, 'openindex);': 0.0, 'hasopennode(visitednodes': 0.0, 'nodes)': 0.0, 'getopennode(visitednodes': 0.0, 'nodes->opennodes[0]);': 0.0, 'asneighborlist': 0.0, 'neighborlistcreate(const': 0.0, '*source)': 0.0, '__asneighborlist));': 0.0, 'list->source': 0.0, 'list;': 0.0, 'neighborlistdestroy(asneighborlist': 0.0, 'list)': 0.0, 'free(list->costs);': 0.0, 'free(list->nodekeys);': 0.0, 'free(list);': 0.0, 'neighborlistgetedgecost(asneighborlist': 0.0, 'list,': 0.0, 'list->costs[index];': 0.0, '*neighborlistgetnodekey(asneighborlist': 0.0, 'list->nodekeys': 0.0, 'list->source->nodesize);': 0.0, 'asneighborlistadd(asneighborlist': 0.0, '*node,': 0.0, 'edgecost)': 0.0, '(list->count': 0.0, 'list->capacity)': 0.0, 'list->capacity': 0.0, '(list->capacity': 0.0, 'list->costs': 0.0, 'realloc(list->costs,': 0.0, 'sizeof(float)': 0.0, 'list->capacity);': 0.0, 'realloc(list->nodekeys,': 0.0, 'list->source->nodesize': 0.0, 'list->costs[list->count]': 0.0, 'edgecost;': 0.0, 'memcpy(list->nodekeys': 0.0, 'list->source->nodesize),': 0.0, 'list->count++;': 0.0, 'aspath': 0.0, 'aspathcreate(const': 0.0, '*context,': 0.0, '*startnodekey,': 0.0, '*goalnodekey)': 0.0, '(!startnodekey': 0.0, '||': 0.0, '!source': 0.0, '!source->nodeneighbors': 0.0, 'source->nodesize': 0.0, 'null;': 0.0, 'visitednodescreate(source,': 0.0, 'context);': 0.0, 'neighborlist': 0.0, 'neighborlistcreate(source);': 0.0, 'current': 0.0, 'getnode(visitednodes,': 0.0, 'startnodekey);': 0.0, 'goalnode': 0.0, 'goalnodekey);': 0.0, 'path': 0.0, 'mark': 0.0, 'goal': 0.0, 'setnodeisgoal(goalnode);': 0.0, 'set': 0.0, 'starting': 0.0, \"node's\": 0.0, 'estimate': 0.0, 'cost': 0.0, 'add': 0.0, 'open': 0.0, 'setnodeestimatedcost(current,': 0.0, 'getpathcostheuristic(current,': 0.0, 'goalnode));': 0.0, 'addnodetoopenset(current,': 0.0, 'nodenull);': 0.0, 'perform': 0.0, 'a*': 0.0, 'algorithm': 0.0, '(hasopennode(visitednodes)': 0.0, '!nodeisgoal((current': 0.0, 'getopennode(visitednodes))))': 0.0, '(source->earlyexit)': 0.0, 'shouldexit': 0.0, 'source->earlyexit(visitednodes->noderecordscount,': 0.0, 'getnodekey(current),': 0.0, 'goalnodekey,': 0.0, '(shouldexit': 0.0, 'setnodeisgoal(current);': 0.0, 'removenodefromopenset(current);': 0.0, 'addnodetoclosedset(current);': 0.0, 'neighborlist->count': 0.0, 'source->nodeneighbors(neighborlist,': 0.0, '(size_t': 0.0, 'n=0;': 0.0, 'n<neighborlist->count;': 0.0, 'n++)': 0.0, 'getnodecost(current)': 0.0, 'neighborlistgetedgecost(neighborlist,': 0.0, 'n);': 0.0, 'neighbor': 0.0, 'neighborlistgetnodekey(neighborlist,': 0.0, 'n));': 0.0, '(!nodehasestimatedcost(neighbor))': 0.0, 'setnodeestimatedcost(neighbor,': 0.0, 'getpathcostheuristic(neighbor,': 0.0, '(nodeisinopenset(neighbor)': 0.0, 'getnodecost(neighbor))': 0.0, 'removenodefromopenset(neighbor);': 0.0, '(nodeisinclosedset(neighbor)': 0.0, 'removenodefromclosedset(neighbor);': 0.0, '(!nodeisinopenset(neighbor)': 0.0, '!nodeisinclosedset(neighbor))': 0.0, 'addnodetoopenset(neighbor,': 0.0, 'current);': 0.0, '(nodeisnull(goalnode))': 0.0, '(nodeisgoal(current))': 0.0, 'count': 0.0, 'n': 0.0, 'current;': 0.0, 'count++;': 0.0, 'getparentnode(n);': 0.0, 'malloc(sizeof(struct': 0.0, '__aspath)': 0.0, '(count': 0.0, 'source->nodesize));': 0.0, 'path->nodesize': 0.0, 'source->nodesize;': 0.0, 'path->count': 0.0, 'path->cost': 0.0, 'getnodecost(current);': 0.0, 'i=count;': 0.0, 'i>0;': 0.0, 'i--)': 0.0, 'memcpy(path->nodekeys': 0.0, '((i': 0.0, 'source->nodesize),': 0.0, 'getnodekey(n),': 0.0, 'source->nodesize);': 0.0, 'neighborlistdestroy(neighborlist);': 0.0, 'visitednodesdestroy(visitednodes);': 0.0, 'path;': 0.0, 'aspathdestroy(aspath': 0.0, 'path)': 0.0, 'free(path);': 0.0, 'aspathcopy(aspath': 0.0, '(path)': 0.0, 'size': 0.0, '(path->count': 0.0, 'path->nodesize);': 0.0, 'newpath': 0.0, 'malloc(size);': 0.0, 'memcpy(newpath,': 0.0, 'path,': 0.0, 'size);': 0.0, 'newpath;': 0.0, 'aspathgetcost(aspath': 0.0, 'path?': 0.0, ':': 0.0, 'infinity;': 0.0, 'aspathgetcount(aspath': 0.0, '*aspathgetnode(aspath': 0.0, '(path': 0.0, 'path->count)?': 0.0, '(path->nodekeys': 0.0, 'path->nodesize))': 0.0}\n",
      "Cosine similarity: 0.9985422732775083\n"
     ]
    }
   ],
   "source": [
    "#Problem 5\n",
    "CODE1 = read_text(\"code1.c\")\n",
    "CODE2 = read_text(\"code2.c\")\n",
    "\n",
    "preprocess_code1 = preprocess_code(CODE1)\n",
    "preprocess_code2 = preprocess_code(CODE2)\n",
    "idf_dict = compute_idf([preprocess_code1, preprocess_code2])\n",
    "\n",
    "code1_tfidf_vector = transform_to_tfidf_vector(CODE1, idf_dict)\n",
    "code2_tfidf_vector = transform_to_tfidf_vector(CODE2, idf_dict)\n",
    "\n",
    "print(\"Original TF-IDF vector:\\n\", code1_tfidf_vector)\n",
    "print(\"Synthetic TF-IDF vector:\\n\", code2_tfidf_vector)\n",
    "\n",
    "similarity_score_code = calculate_cosine_similarity(code1_tfidf_vector, code2_tfidf_vector)\n",
    "print(\"Cosine similarity:\", similarity_score_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
