{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import tokenize\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selecciona un cuerpo de texto de interés (extensión .txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text from the specified file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_text(file_path, text):\n",
    "    \"\"\"\n",
    "    Saves the given text to a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file where the text will be saved.\n",
    "    text (str): The text to save.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Genera un cuerpo de texto sintético utilizando herramientas como MarkovifyLinks to an external site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_text(model, num_sentences=100):\n",
    "    \"\"\"\n",
    "    Generates synthetic text using a Markov model.\n",
    "\n",
    "    Parameters:\n",
    "    model (markovify.Text): The Markov model generated from the original text.\n",
    "    num_sentences (int): The number of sentences to generate.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated synthetic text as a single string.\n",
    "    \"\"\"\n",
    "    synthetic_text = \"\"\n",
    "    for _ in range(num_sentences):\n",
    "        sentence = model.make_sentence()\n",
    "        if sentence is not None:\n",
    "            synthetic_text += sentence + \" \"\n",
    "    return synthetic_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transforma el cuerpo de texto original y el sintético a una representación vectorial, por ejemplo tf–idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by converting to lowercase and removing stop words.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    list: The preprocessed words in the text.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "def compute_tf(word_dict, word_count):\n",
    "    \"\"\"\n",
    "    Computes the term frequency for each word in the word dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    word_dict (dict): A dictionary of words and their counts in the text.\n",
    "    word_count (int): The total number of words in the text.\n",
    "\n",
    "    Returns:\n",
    "    dict: The term frequency for each word.\n",
    "    \"\"\"\n",
    "    tf_dict = {}\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(word_count)\n",
    "    return tf_dict\n",
    "\n",
    "def compute_idf(documents):\n",
    "    \"\"\"\n",
    "    Computes the inverse document frequency for each word in the documents.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): A list of lists, where each sublist contains the words in a document.\n",
    "\n",
    "    Returns:\n",
    "    dict: The inverse document frequency for each word.\n",
    "    \"\"\"\n",
    "    N = len(documents)\n",
    "    idf_dict = dict.fromkeys(documents[0], 0)\n",
    "    for document in documents:\n",
    "        for word in document:\n",
    "            if word in idf_dict:\n",
    "                idf_dict[word] += 1\n",
    "\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "    return idf_dict\n",
    "\n",
    "def compute_tfidf(tf, idf):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF score for each term in a document.\n",
    "\n",
    "    Parameters:\n",
    "    tf (dict): A dictionary mapping terms to their TF values.\n",
    "    idf (dict): A dictionary mapping terms to their IDF values.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping terms to their TF-IDF values.\n",
    "    \"\"\"\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val * idf.get(word, 0.0)\n",
    "    return tfidf\n",
    "\n",
    "def transform_to_tfidf_vector(text, idf_dict):\n",
    "    \"\"\"\n",
    "    Transforms the given text into a TF-IDF vector representation.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to transform.\n",
    "    idf_dict (dict): The inverse document frequency for each word.\n",
    "\n",
    "    Returns:\n",
    "    dict: The TF-IDF vector representation of the text.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    word_count = len(words)\n",
    "    word_dict = Counter(words)\n",
    "    tf = compute_tf(word_dict, word_count)\n",
    "    tfidf = compute_tfidf(tf, idf_dict)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utiliza una métrica de similitud como la distancia del coseno para obtener un valor de similitud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(tfidf1, tfidf2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two TF-IDF vectors.\n",
    "\n",
    "    Parameters:\n",
    "    tfidf1 (dict): The TF-IDF vector representation of the first text.\n",
    "    tfidf2 (dict): The TF-IDF vector representation of the second text.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity score between the two texts.\n",
    "    \"\"\"\n",
    "    common_words = set(tfidf1.keys()).intersection(set(tfidf2.keys()))\n",
    "\n",
    "    dot_product = 0\n",
    "    for word in common_words:\n",
    "        dot_product += tfidf1[word] * tfidf2[word]\n",
    "    \n",
    "    magnitude1 = 0\n",
    "    for val in tfidf1.values():\n",
    "        magnitude1 += val ** 2\n",
    "    magnitude1 = math.sqrt(magnitude1)\n",
    "    \n",
    "    magnitude2 = 0\n",
    "    for val in tfidf2.values():\n",
    "        magnitude2 += val ** 2\n",
    "    magnitude2 = math.sqrt(magnitude2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (magnitude1 * magnitude2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compara contra otros cuerpos de texto.(CODIGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_code(code):\n",
    "    \"\"\"\n",
    "    Preprocesses the code by tokenizing it.\n",
    "\n",
    "    Parameters:\n",
    "    code (str): The code to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    list: The tokens in the code.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    try:\n",
    "        bytes_io = BytesIO(code.encode('utf-8'))\n",
    "        for token in tokenize.tokenize(bytes_io.readline):\n",
    "            # Exclude specific token types that are not useful for analysis\n",
    "            if token.type not in [\n",
    "                tokenize.ENCODING, \n",
    "                tokenize.ENDMARKER, \n",
    "                tokenize.NEWLINE, \n",
    "                tokenize.NL, \n",
    "                tokenize.INDENT, \n",
    "                tokenize.DEDENT,\n",
    "                tokenize.COMMENT\n",
    "            ]:\n",
    "                tokens.append(token.string.lower())\n",
    "    except tokenize.TokenError:\n",
    "        pass\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF vector:\n",
      " {'kaladin': -0.1809249289654102, '(apodado': 0.0025297342356202382, 'kal)': 0.0025297342356202382, 'es': 0.0, 'niño': 0.0025297342356202382, 'que': -0.15510292029504832, 'vive': 0.0025297342356202382, 'en': -0.02529734235620238, 'pueblo': -0.006688253517329598, 'llamado': 0.0025297342356202382, 'piedralar': 0.0025297342356202382, 'su': -0.09455995480771913, 'madre': 0.0025297342356202382, 'hesina,': 0.0025297342356202382, 'hermano': -0.02521078568971583, 'tien': -0.0164327868511423, 'y': -0.1378608001800606, 'padre': -0.05324399146139195, 'lirin.': 0.0025297342356202382, 'cirujano': 0.0025297342356202382, 'experto': 0.0025297342356202382, 'cura': 0.0025297342356202382, 'las': -0.02529734235620238, 'personas': 0.0025297342356202382, 'heridas': 0.0025297342356202382, 'junto': 0.0025297342356202382, 'kaladin,': -0.0164327868511423, 'ya': 0.0, 'soporta': -0.00801906780049715, 'ver': -0.00801906780049715, 'la': -0.09839996518972963, 'sangre.': -0.00801906780049715, 'día': -0.0164327868511423, 'laral,': 0.0025297342356202382, 'una': -0.04941697447266806, 'amiga': 0.0025297342356202382, 'suya,': 0.0025297342356202382, 'se': -0.08818758791662372, 'encontraban': 0.0025297342356202382, 'jugando': 0.0025297342356202382, 'entre': 0.0, 'rocas.': 0.0025297342356202382, 'laral': 0.0025297342356202382, 'propuso': 0.0025297342356202382, 'ir': 0.0025297342356202382, 'hablar': 0.0025297342356202382, 'los': -0.010978667129753825, 'otros': 0.0025297342356202382, 'chicos,': 0.0025297342356202382, 'pero': 0.0, 'hubo': 0.0025297342356202382, 'discursión': 0.0025297342356202382, 'jost,': -0.0050594684712404765, 'uno': -0.0050594684712404765, 'chicos.': -0.0050594684712404765, 'jost': -0.010978667129753825, 'retó': -0.0050594684712404765, 'batalla': -0.0054893335648769125, 'palos.': -0.0054893335648769125, 'al': 0.0025297342356202382, 'coger': 0.0025297342356202382, 'el': -0.03676830329456687, 'palo': 0.0025297342356202382, 'sintió': 0.0025297342356202382, 'buena': 0.0025297342356202382, 'sensación,': 0.0025297342356202382, 'después': 0.0, 'del': 0.0025297342356202382, 'combate': 0.0025297342356202382, 'pide': 0.0025297342356202382, 'le': -0.04092496906432897, 'enseñe': 0.0025297342356202382, 'combatir,': 0.0025297342356202382, 'este': -0.022767608120582143, 'quiere.': 0.0025297342356202382, 'todo': -0.004572127622245869, 'marchaba': -0.004572127622245869, 'bien': -0.004572127622245869, 'para': -0.004572127622245869, 'familia': -0.029414642635653496, 'hasta': -0.0025297342356202382, 'brillante': 0.0025297342356202382, 'señor': 0.0025297342356202382, 'wistiow,': 0.0025297342356202382, 'murió': 0.0025297342356202382, 'manos': 0.0025297342356202382, 'linir': 0.0025297342356202382, 'cuando': -0.030356810827442855, 'intentaba': 0.0025297342356202382, 'salvarle': 0.0025297342356202382, 'vida.': 0.0025297342356202382, 'unos': 0.0025297342356202382, 'días': 0.0025297342356202382, 'wistiow': 0.0025297342356202382, 'fue': 0.0, 'sustituido': 0.0025297342356202382, 'por': 0.0025297342356202382, 'roshone,': 0.0025297342356202382, 'ojos': 0.0025297342356202382, 'claros': 0.0025297342356202382, 'arrogante': 0.0025297342356202382, 'odia': 0.0025297342356202382, 'kaladin.': 0.0025297342356202382, 'rumorea': -0.0025297342356202382, 'robó': 0.0025297342356202382, 'esferas': 0.0, 'wistow,': 0.0025297342356202382, 'verdad': 0.0025297342356202382, 'lindir': 0.0, 'wistow': 0.0, 'firmaron': 0.0025297342356202382, 'tratado': 0.0025297342356202382, 'si': -0.012028601700745729, 'moría,': 0.0025297342356202382, 'quedaba': 0.0025297342356202382, 'esferas.': -0.006688253517329598, 'ha': 0.0025297342356202382, 'decidido': 0.0025297342356202382, 'quiere': 0.0025297342356202382, 'ser': 0.0025297342356202382, 'guerrero.': 0.0025297342356202382, 'acompaña': 0.0025297342356202382, 'charla': -0.00801906780049715, 'roshone.': -0.00801906780049715, 'enfada': 0.0025297342356202382, 'roshone': -0.013376507034659197, 'cena,': 0.0025297342356202382, 'dice': -0.03102058405900967, 'dejará': 0.0025297342356202382, 'empaz': 0.0025297342356202382, 'dan': -0.0025297342356202382, 'más': -0.0025297342356202382, 'mitad': -0.0025297342356202382, 'lirin': -0.004009533900248575, 'vaya,': -0.007101861857866107, 'lo': -0.007101861857866107, 'hace.': -0.007101861857866107, 'marcha': -0.0160381356009943, 'encuentra': 0.0, 'rillir,': 0.0, 'hijo': 0.0, 'laral.': -0.0160381356009943, 'rillir': -0.004009533900248575, 'empieza': -0.004009533900248575, 'insultar': -0.004009533900248575, 'vuelve': -0.009498867465125488, 'recogerle': -0.009498867465125488, 'él': 0.0025297342356202382, 'han': 0.0025297342356202382, 'llegado': 0.0025297342356202382, 'ningún': 0.0025297342356202382, 'acuerdo': 0.0025297342356202382, 'también': 0.0025297342356202382, 'cuenta': 0.0025297342356202382, 'realidad': 0.0025297342356202382, 'están': 0.0025297342356202382, 'robadas.': 0.0025297342356202382}\n",
      "Synthetic TF-IDF vector:\n",
      " {'su': -0.1725689881574577, 'padre': -0.11569846769107425, 'vuelve': -0.08385369494616353, 'recogerle': -0.08385369494616353, 'y': -0.2170177787221096, 'le': -0.10074033586900807, 'dice': -0.10883471334044945, 'kaladin': -0.2413293219604118, 'una': -0.08389250932782559, 'batalla': -0.01491030876605972, 'palos.': -0.01491030876605972, 'día': -0.04742498212677871, 'kaladin,': -0.04742498212677871, 'hermano': -0.048505510013505296, 'tien': -0.04742498212677871, 'soporta': -0.0462860195969216, 'ver': -0.0462860195969216, 'la': -0.11507769576229705, 'sangre.': -0.0462860195969216, 'jost': -0.013046520170302256, 'retó': -0.012024858150977986, 'charla': -0.0462860195969216, 'roshone.': -0.0462860195969216, 'lirin': -0.006806767587782589, 'rillir': -0.006806767587782589, 'empieza': -0.006806767587782589, 'insultar': -0.006806767587782589, 'familia': -0.027464601271331983, 'jost,': -0.012024858150977986, 'uno': -0.012024858150977986, 'los': -0.013046520170302256, 'chicos.': -0.012024858150977986, 'que': -0.06670514688608191, 'se': -0.12351179831817095, 'vaya,': -0.03134675580882165, 'este': -0.03349781913486724, 'lo': -0.03134675580882165, 'hace.': -0.03134675580882165, 'cuando': -0.03092106381680053, 'marcha': -0.043563312561808565, 'laral.': -0.043563312561808565, 'en': -0.005153510636133422, 'el': -0.024967819337574533, 'pueblo': -0.0034062852486028065, 'rumorea': -0.002576755318066711, 'todo': -0.009314222814091956, 'marchaba': -0.009314222814091956, 'bien': -0.009314222814091956, 'para': -0.009314222814091956, 'hasta': -0.002576755318066711, 'si': -0.004084060552669553, 'dan': -0.002576755318066711, 'más': -0.002576755318066711, 'mitad': -0.002576755318066711, 'las': -0.005153510636133422, 'esferas.': -0.0034062852486028065, 'encuentra': 0.0, 'rillir,': 0.0, 'hijo': 0.0, 'roshone': -0.0011354284162009357}\n",
      "Cosine similarity: 0.8813288248013081\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_TEXT = read_text(\"texto.txt\")\n",
    "SYNTETIC_TEXT = \"texto_sintetico.txt\"\n",
    "\n",
    "#Problem 2\n",
    "markov_model = markovify.Text(ORIGINAL_TEXT) # Build the Markov model\n",
    "synthetic_text = generate_synthetic_text(markov_model, num_sentences=100)\n",
    "\n",
    "save_text(SYNTETIC_TEXT, synthetic_text)\n",
    "\n",
    "#Problem 3\n",
    "original_words = preprocess_text(ORIGINAL_TEXT)\n",
    "synthetic_words = preprocess_text(synthetic_text)\n",
    "idf_dict = compute_idf([original_words, synthetic_words])\n",
    "\n",
    "original_tfidf_vector = transform_to_tfidf_vector(ORIGINAL_TEXT, idf_dict)\n",
    "synthetic_tfidf_vector = transform_to_tfidf_vector(synthetic_text, idf_dict)\n",
    "\n",
    "print(\"Original TF-IDF vector:\\n\", original_tfidf_vector)\n",
    "print(\"Synthetic TF-IDF vector:\\n\", synthetic_tfidf_vector)\n",
    "\n",
    "#Problem 4\n",
    "similarity_score = calculate_cosine_similarity(original_tfidf_vector, synthetic_tfidf_vector)\n",
    "print(\"Cosine similarity:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF vector:\n",
      " {'#': 0.0, 'python': 0.0, 'program': 0.0, \"dijkstra's\": 0.0, 'single': 0.0, 'source': 0.0, 'shortest': 0.0, 'path': 0.0, 'algorithm.': 0.0, 'adjacency': 0.0, 'matrix': 0.0, 'representation': 0.0, 'graph': -0.008214839137674545, 'library': 0.0, 'int_max': 0.0, 'import': 0.002272613706753919, 'sys': -0.001329393797075949, 'class': 0.0, 'graph():': 0.0, 'def': -0.018180909654031352, '__init__(self,': 0.0, 'vertices):': 0.0, 'self.v': 0.0, '=': -0.1345606736883655, 'vertices': -0.009090454827015676, 'self.graph': 0.0, '[[0': 0.0, 'column': 0.002272613706753919, 'range(vertices)]': 0.0, 'row': 0.002272613706753919, 'printsolution(self,': 0.0, 'dist):': 0.0, 'print(\"vertex': 0.0, '\\\\tdistance': 0.0, 'source\")': 0.0, 'node': -0.008214839137674545, 'range(self.v):': 0.0, 'print(node,': 0.0, '\"\\\\t\",': 0.0, 'dist[node])': 0.0, 'utility': 0.0, 'function': 0.0, 'vertex': 0.0, 'minimum': 0.0, 'distance': 0.0, 'value,': 0.0, 'set': 0.0, 'included': 0.0, 'tree': 0.0, 'mindistance(self,': 0.0, 'dist,': 0.0, 'sptset):': 0.0, 'initialize': 0.0, 'min': -0.003988181391227847, 'sys.maxsize': 0.0, 'search': 0.0, 'nearest': 0.0, 'u': -0.00600846381556823, 'dist[u]': 0.0, '<': -0.002272613706753919, 'sptset[u]': 0.0, '==': -0.013635682240523516, 'false:': 0.0, 'min_index': 0.0, 'return': -0.001329393797075949, 'implements': 0.0, 'algorithm': 0.0, 'represented': 0.0, 'using': 0.0, 'dijkstra(self,': 0.0, 'src):': 0.0, 'dist': -0.01227411263542027, '[sys.maxsize]': 0.0, '*': 0.0, 'dist[src]': 0.0, '0': -0.029246848615985796, 'sptset': -0.003602007503829868, '[false]': 0.0, 'cout': 0.002272613706753919, 'pick': 0.0, 'processed.': 0.0, 'x': -0.008214839137674545, 'equal': 0.0, 'src': 0.0, 'iteration': 0.0, 'self.mindistance(dist,': 0.0, 'sptset)': 0.0, 'sptset[x]': 0.0, 'true': 0.002272613706753919, 'update': 0.0, 'value': 0.0, 'adjacent': 0.0, 'picked': 0.0, 'current': 0.0, 'greater': 0.0, 'new': 0.0, 'y': -0.004107419568837273, 'self.graph[x][y]': 0.0, '>': -0.007204015007659736, 'sptset[y]': 0.0, 'false': -0.001329393797075949, '\\\\': 0.0, 'dist[y]': 0.0, 'dist[x]': 0.0, '+': -0.008214839137674545, 'self.graph[x][y]:': 0.0, 'self.printsolution(dist)': 0.0, \"driver's\": 0.0, 'code': 0.0, '__name__': 0.0, '\"__main__\":': 0.0, 'g': -0.005589338007339099, 'graph(9)': 0.0, 'g.graph': 0.0, '[[0,': 0.0, '4,': 0.0, '0,': 0.0, '8,': 0.0, '0],': 0.0, '[4,': 0.0, '11,': 0.0, '[0,': 0.0, '7,': 0.0, '2],': 0.0, '9,': 0.0, '14,': 0.0, '10,': 0.0, '2,': 0.0, '1,': 0.0, '6],': 0.0, '[8,': 0.0, '7],': 0.0, '6,': 0.0, '0]': 0.0, ']': -0.011463959217922886, 'g.dijkstra(0)': 0.0, 'contributed': 0.0, 'divyanshu': 0.0, 'mehta': 0.0, 'updated': 0.0, 'pranav': 0.0, 'singh': 0.0, 'sambyal': 0.0}\n",
      "Synthetic TF-IDF vector:\n",
      " {'class': 0.0, 'node():': 0.0, '\"\"\"a': 0.0, 'node': -0.018244120900417982, 'a*': 0.0, 'pathfinding\"\"\"': 0.0, 'def': -0.013459168554562043, '__init__(self,': 0.0, 'parent=none,': 0.0, 'position=none):': 0.0, 'self.parent': 0.0, '=': -0.24520391620583404, 'parent': 0.0, 'self.position': 0.0, 'position': 0.0, 'self.g': 0.0, '0': -0.07577915266399232, 'self.h': 0.0, 'self.f': 0.0, '__eq__(self,': 0.0, 'other):': 0.0, 'return': -0.0029524158357390616, '==': -0.016823960693202557, 'other.position': 0.0, 'astar(maze,': 0.0, 'start,': 0.0, 'end):': 0.0, '\"\"\"returns': 0.0, 'list': 0.0, 'tuples': 0.0, 'path': 0.0, 'given': 0.0, 'start': 0.0, 'end': 0.0, 'maze\"\"\"': 0.0, '#': 0.0, 'create': 0.0, 'start_node': 0.0, 'node(none,': 0.0, 'start)': 0.0, 'start_node.g': 0.0, 'start_node.h': 0.0, 'start_node.f': 0.0, 'end_node': 0.0, 'end)': 0.0, 'end_node.g': 0.0, 'end_node.h': 0.0, 'end_node.f': 0.0, 'initialize': 0.0, 'open': 0.0, 'closed': 0.0, 'open_list': 0.0, '[]': 0.0, 'closed_list': 0.0, 'add': 0.0, 'open_list.append(start_node)': 0.0, 'loop': 0.0, 'len(open_list)': 0.0, '>': -0.010666138724933103, '0:': 0.0, 'current': 0.0, 'current_node': 0.0, 'open_list[0]': 0.0, 'current_index': 0.0, 'index,': 0.0, 'item': 0.0, 'enumerate(open_list):': 0.0, 'item.f': 0.0, '<': -0.005047188207960767, 'current_node.f:': 0.0, 'index': 0.0, 'pop': 0.0, 'list,': 0.0, 'open_list.pop(current_index)': 0.0, 'closed_list.append(current_node)': 0.0, 'goal': 0.0, 'end_node:': 0.0, 'none:': 0.0, 'path.append(current.position)': 0.0, 'current.parent': 0.0, 'path[::-1]': 0.0, 'reversed': 0.0, 'generate': 0.0, 'children': 0.0, 'new_position': 0.0, '[(0,': 0.0, '-1),': 0.0, '(0,': 0.0, '1),': 0.0, '(-1,': 0.0, '0),': 0.0, '(1,': 0.0, '1)]:': 0.0, 'adjacent': 0.0, 'squares': 0.0, 'node_position': 0.0, '(current_node.position[0]': 0.0, '+': -0.01520343408368165, 'new_position[0],': 0.0, 'current_node.position[1]': 0.0, 'new_position[1])': 0.0, 'make': 0.0, 'sure': 0.0, 'range': -0.002666534681233276, 'node_position[0]': 0.0, '(len(maze)': 0.0, '-': 0.0, '1)': 0.0, 'node_position[1]': 0.0, '(len(maze[len(maze)-1])': 0.0, '-1)': 0.0, 'continue': 0.0, 'walkable': 0.0, 'terrain': 0.0, 'maze[node_position[0]][node_position[1]]': 0.0, '!=': 0.0, 'new': 0.0, 'new_node': 0.0, 'node(current_node,': 0.0, 'node_position)': 0.0, 'append': 0.0, 'children.append(new_node)': 0.0, 'child': 0.0, 'children:': 0.0, 'closed_child': 0.0, 'closed_list:': 0.0, 'closed_child:': 0.0, 'f,': 0.0, 'g,': 0.0, 'h': 0.0, 'values': 0.0, 'child.g': 0.0, 'current_node.g': 0.0, '1': -0.0068767314176121745, 'child.h': 0.0, '((child.position[0]': 0.0, 'end_node.position[0])': 0.0, '**': 0.0, '2)': 0.0, '((child.position[1]': 0.0, 'end_node.position[1])': 0.0, 'child.f': 0.0, 'open_node': 0.0, 'open_list:': 0.0, 'open_node.g:': 0.0, 'open_list.append(child)': 0.0, 'main():': 0.0, 'maze': 0.0, '[[0,': 0.0, '0,': 0.0, '1,': 0.0, '0],': 0.0, '[0,': 0.0, '0]]': 0.0, '0)': 0.0, '(7,': 0.0, '6)': 0.0, 'print(path)': 0.0, '__name__': 0.0, \"'__main__':\": 0.0, 'main()': 0.0}\n",
      "Cosine similarity: 0.9765075817014731\n"
     ]
    }
   ],
   "source": [
    "#Problem 5\n",
    "CODE1 = read_text(\"code1.py\")\n",
    "CODE2 = read_text(\"code2.py\")\n",
    "\n",
    "preprocess_code1 = preprocess_code(CODE1)\n",
    "preprocess_code2 = preprocess_code(CODE2)\n",
    "idf_dict = compute_idf([preprocess_code1, preprocess_code2])\n",
    "\n",
    "code1_tfidf_vector = transform_to_tfidf_vector(CODE1, idf_dict)\n",
    "code2_tfidf_vector = transform_to_tfidf_vector(CODE2, idf_dict)\n",
    "\n",
    "print(\"Original TF-IDF vector:\\n\", code1_tfidf_vector)\n",
    "print(\"Synthetic TF-IDF vector:\\n\", code2_tfidf_vector)\n",
    "\n",
    "similarity_score_code = calculate_cosine_similarity(code1_tfidf_vector, code2_tfidf_vector)\n",
    "print(\"Cosine similarity:\", similarity_score_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
