{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import tokenize\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selecciona un cuerpo de texto de interés (extensión .txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text from the specified file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_text(file_path, text):\n",
    "    \"\"\"\n",
    "    Saves the given text to a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file where the text will be saved.\n",
    "    text (str): The text to save.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Genera un cuerpo de texto sintético utilizando herramientas como MarkovifyLinks to an external site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_text(model, num_sentences=100):\n",
    "    \"\"\"\n",
    "    Generates synthetic text using a Markov model.\n",
    "\n",
    "    Parameters:\n",
    "    model (markovify.Text): The Markov model generated from the original text.\n",
    "    num_sentences (int): The number of sentences to generate.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated synthetic text as a single string.\n",
    "    \"\"\"\n",
    "    synthetic_text = \"\"\n",
    "    for _ in range(num_sentences):\n",
    "        sentence = model.make_sentence()\n",
    "        if sentence is not None:\n",
    "            synthetic_text += sentence + \" \"\n",
    "    return synthetic_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transforma el cuerpo de texto original y el sintético a una representación vectorial, por ejemplo tf–idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by converting to lowercase and removing stop words.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    list: The preprocessed words in the text.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "def compute_tf(word_dict, word_count):\n",
    "    \"\"\"\n",
    "    Computes the term frequency for each word in the word dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    word_dict (dict): A dictionary of words and their counts in the text.\n",
    "    word_count (int): The total number of words in the text.\n",
    "\n",
    "    Returns:\n",
    "    dict: The term frequency for each word.\n",
    "    \"\"\"\n",
    "    tf_dict = {}\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(word_count)\n",
    "    return tf_dict\n",
    "\n",
    "def compute_idf(documents):\n",
    "    \"\"\"\n",
    "    Computes the inverse document frequency for each word in the documents.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): A list of lists, where each sublist contains the words in a document.\n",
    "\n",
    "    Returns:\n",
    "    dict: The inverse document frequency for each word.\n",
    "    \"\"\"\n",
    "    N = len(documents)\n",
    "    idf_dict = dict.fromkeys(documents[0], 0)\n",
    "    for document in documents:\n",
    "        for word in document:\n",
    "            if word in idf_dict:\n",
    "                idf_dict[word] += 1\n",
    "\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "    return idf_dict\n",
    "\n",
    "def compute_tfidf(tf, idf):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF score for each term in a document.\n",
    "\n",
    "    Parameters:\n",
    "    tf (dict): A dictionary mapping terms to their TF values.\n",
    "    idf (dict): A dictionary mapping terms to their IDF values.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping terms to their TF-IDF values.\n",
    "    \"\"\"\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val * idf.get(word, 0.0)\n",
    "    return tfidf\n",
    "\n",
    "def transform_to_tfidf_vector(text, idf_dict):\n",
    "    \"\"\"\n",
    "    Transforms the given text into a TF-IDF vector representation.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to transform.\n",
    "    idf_dict (dict): The inverse document frequency for each word.\n",
    "\n",
    "    Returns:\n",
    "    dict: The TF-IDF vector representation of the text.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    word_count = len(words)\n",
    "    word_dict = Counter(words)\n",
    "    tf = compute_tf(word_dict, word_count)\n",
    "    tfidf = compute_tfidf(tf, idf_dict)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utiliza una métrica de similitud como la distancia del coseno para obtener un valor de similitud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(tfidf1, tfidf2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two TF-IDF vectors.\n",
    "\n",
    "    Parameters:\n",
    "    tfidf1 (dict): The TF-IDF vector representation of the first text.\n",
    "    tfidf2 (dict): The TF-IDF vector representation of the second text.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity score between the two texts.\n",
    "    \"\"\"\n",
    "    common_words = set(tfidf1.keys()).intersection(set(tfidf2.keys()))\n",
    "\n",
    "    dot_product = 0\n",
    "    for word in common_words:\n",
    "        dot_product += tfidf1[word] * tfidf2[word]\n",
    "    \n",
    "    magnitude1 = 0\n",
    "    for val in tfidf1.values():\n",
    "        magnitude1 += val ** 2\n",
    "    magnitude1 = math.sqrt(magnitude1)\n",
    "    \n",
    "    magnitude2 = 0\n",
    "    for val in tfidf2.values():\n",
    "        magnitude2 += val ** 2\n",
    "    magnitude2 = math.sqrt(magnitude2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (magnitude1 * magnitude2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compara contra otros cuerpos de texto.(CODIGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_code(code):\n",
    "    \"\"\"\n",
    "    Preprocesses the code by tokenizing it.\n",
    "\n",
    "    Parameters:\n",
    "    code (str): The code to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    list: The tokens in the code.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    try:\n",
    "        bytes_io = BytesIO(code.encode('utf-8'))\n",
    "        for token in tokenize.tokenize(bytes_io.readline):\n",
    "            # Exclude specific token types that are not useful for analysis\n",
    "            if token.type not in [\n",
    "                tokenize.ENCODING, \n",
    "                tokenize.ENDMARKER, \n",
    "                tokenize.NEWLINE, \n",
    "                tokenize.NL, \n",
    "                tokenize.INDENT, \n",
    "                tokenize.DEDENT,\n",
    "                tokenize.COMMENT\n",
    "            ]:\n",
    "                tokens.append(token.string.lower())\n",
    "    except tokenize.TokenError:\n",
    "        pass\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF vector:\n",
      " {'kaladin': -0.18586206655536278, '(apodado': 0.0025297342356202382, 'kal)': 0.0025297342356202382, 'es': 0.0, 'niño': 0.0025297342356202382, 'que': -0.16921765255611218, 'vive': 0.0025297342356202382, 'en': -0.032696340679344064, 'pueblo': -0.010978667129753825, 'llamado': 0.0025297342356202382, 'piedralar': 0.0025297342356202382, 'su': -0.09398177590856059, 'madre': 0.0025297342356202382, 'hesina,': 0.0025297342356202382, 'hermano': -0.027653963258849515, 'tien': -0.018138004742978105, 'y': -0.13586968210512232, 'padre': -0.055117242447889826, 'lirin.': 0.0025297342356202382, 'cirujano': 0.0025297342356202382, 'experto': 0.0025297342356202382, 'cura': 0.0025297342356202382, 'las': -0.03794601353430357, 'personas': 0.0025297342356202382, 'heridas': 0.0025297342356202382, 'junto': 0.0025297342356202382, 'kaladin,': -0.018138004742978105, 'ya': 0.0, 'soporta': -0.008913675311566439, 'ver': -0.008913675311566439, 'la': -0.11060460755430025, 'sangre.': -0.008913675311566439, 'día': -0.018138004742978105, 'laral,': 0.0025297342356202382, 'una': -0.043757213007269535, 'amiga': 0.0025297342356202382, 'suya,': 0.0025297342356202382, 'se': -0.09512690621960532, 'encontraban': 0.0025297342356202382, 'jugando': 0.0025297342356202382, 'entre': 0.0, 'rocas.': 0.0025297342356202382, 'laral': 0.0025297342356202382, 'propuso': 0.0025297342356202382, 'ir': 0.0025297342356202382, 'hablar': 0.0025297342356202382, 'los': -0.012443416731667337, 'otros': 0.0025297342356202382, 'chicos,': 0.0025297342356202382, 'pero': 0.0, 'hubo': 0.0025297342356202382, 'discursión': 0.0025297342356202382, 'jost,': -0.005873860994285037, 'uno': -0.005873860994285037, 'chicos.': -0.005873860994285037, 'jost': -0.009144255244491738, 'retó': -0.004009533900248575, 'batalla': -0.004009533900248575, 'palos.': -0.004009533900248575, 'al': 0.0025297342356202382, 'coger': 0.0025297342356202382, 'el': -0.050015328903744535, 'palo': 0.0025297342356202382, 'sintió': 0.0025297342356202382, 'buena': 0.0025297342356202382, 'sensación,': 0.0025297342356202382, 'después': 0.0, 'del': 0.0025297342356202382, 'combate': 0.0025297342356202382, 'pide': 0.0025297342356202382, 'le': -0.041783954466123624, 'enseñe': 0.0025297342356202382, 'combatir,': 0.0025297342356202382, 'este': -0.022767608120582143, 'quiere.': 0.0025297342356202382, 'todo': -0.005873860994285037, 'marchaba': -0.005873860994285037, 'bien': -0.005873860994285037, 'para': -0.005873860994285037, 'familia': -0.0336143809196211, 'hasta': -0.005873860994285037, 'brillante': 0.0025297342356202382, 'señor': 0.0025297342356202382, 'wistiow,': 0.0025297342356202382, 'murió': 0.0025297342356202382, 'manos': 0.0025297342356202382, 'linir': 0.0025297342356202382, 'cuando': -0.03500577040581563, 'intentaba': 0.0025297342356202382, 'salvarle': 0.0025297342356202382, 'vida.': 0.0025297342356202382, 'unos': 0.0025297342356202382, 'días': 0.0025297342356202382, 'wistiow': 0.0025297342356202382, 'fue': 0.0, 'sustituido': 0.0025297342356202382, 'por': 0.0025297342356202382, 'roshone,': 0.0025297342356202382, 'ojos': 0.0025297342356202382, 'claros': 0.0025297342356202382, 'arrogante': 0.0025297342356202382, 'odia': 0.0025297342356202382, 'kaladin.': 0.0025297342356202382, 'rumorea': -0.0050594684712404765, 'robó': 0.0025297342356202382, 'esferas': 0.0, 'wistow,': 0.0025297342356202382, 'verdad': 0.0025297342356202382, 'lindir': 0.0, 'wistow': 0.0, 'firmaron': 0.0025297342356202382, 'tratado': 0.0025297342356202382, 'si': -0.021305585573598324, 'moría,': 0.0025297342356202382, 'quedaba': 0.0025297342356202382, 'esferas.': -0.013662789612420375, 'ha': 0.0025297342356202382, 'decidido': 0.0025297342356202382, 'quiere': 0.0025297342356202382, 'ser': 0.0025297342356202382, 'guerrero.': 0.0025297342356202382, 'acompaña': 0.0025297342356202382, 'charla': -0.006831394806210187, 'roshone.': -0.006831394806210187, 'enfada': 0.0025297342356202382, 'roshone': -0.018288510488983476, 'cena,': 0.0025297342356202382, 'dice': -0.02889478828045904, 'dejará': 0.0025297342356202382, 'empaz': 0.0025297342356202382, 'dan': -0.006539268135868814, 'más': -0.006539268135868814, 'mitad': -0.006539268135868814, 'lirin': -0.004009533900248575, 'vaya,': -0.007101861857866107, 'lo': -0.007101861857866107, 'hace.': -0.007101861857866107, 'marcha': -0.017827350623132877, 'encuentra': -0.0025297342356202382, 'rillir,': -0.0025297342356202382, 'hijo': -0.0025297342356202382, 'laral.': -0.017827350623132877, 'rillir': -0.0050594684712404765, 'empieza': -0.0050594684712404765, 'insultar': -0.0050594684712404765, 'vuelve': -0.008581661522494444, 'recogerle': -0.008581661522494444, 'él': 0.0025297342356202382, 'han': 0.0025297342356202382, 'llegado': 0.0025297342356202382, 'ningún': 0.0025297342356202382, 'acuerdo': 0.0025297342356202382, 'también': 0.0025297342356202382, 'cuenta': 0.0025297342356202382, 'realidad': 0.0025297342356202382, 'están': 0.0025297342356202382, 'robadas.': 0.0025297342356202382}\n",
      "Synthetic TF-IDF vector:\n",
      " {'su': -0.13781119739496883, 'padre': -0.1108412802254812, 'vuelve': -0.04793833347937773, 'recogerle': -0.04793833347937773, 'y': -0.16559707710560365, 'le': -0.09044671489175692, 'dice': -0.06725426426134706, 'kaladin': -0.22989841172582662, 'que': -0.09137638241324277, 'se': -0.14613277030881267, 'vaya,': -0.02578678077239457, 'este': -0.027556309930517702, 'lo': -0.02578678077239457, 'hace.': -0.02578678077239457, 'día': -0.05572675463337004, 'kaladin,': -0.05572675463337004, 'hermano': -0.056642232594068934, 'tien': -0.05572675463337004, 'soporta': -0.054772308642326704, 'ver': -0.054772308642326704, 'la': -0.16819316581991564, 'sangre.': -0.054772308642326704, 'cuando': -0.04399807839997011, 'marcha': -0.052282658249493665, 'laral.': -0.052282658249493665, 'todo': -0.014765485435175233, 'marchaba': -0.014765485435175233, 'bien': -0.014765485435175233, 'para': -0.014765485435175233, 'familia': -0.03755490467676323, 'encuentra': -0.0021197161485013616, 'rillir,': -0.0021197161485013616, 'el': -0.07264203937008687, 'hijo': -0.0021197161485013616, 'roshone': -0.0038310794143589237, 'una': -0.04155374071108287, 'batalla': -0.005599451012579561, 'palos.': -0.005599451012579561, 'jost': -0.006385132357264873, 'retó': -0.005599451012579561, 'charla': -0.022896662714392554, 'roshone.': -0.022896662714392554, 'lirin': -0.005599451012579561, 'hasta': -0.014765485435175233, 'si': -0.021819583730487715, 'dan': -0.020091084772180027, 'más': -0.020091084772180027, 'mitad': -0.020091084772180027, 'las': -0.02331687763351498, 'esferas.': -0.020988607488193176, 'rillir': -0.009892008693006354, 'empieza': -0.009892008693006354, 'insultar': -0.009892008693006354, 'en': -0.01278523576411456, 'pueblo': -0.010732458488719591, 'rumorea': -0.009892008693006354, 'jost,': -0.014765485435175233, 'uno': -0.014765485435175233, 'los': -0.015639890754480967, 'chicos.': -0.014765485435175233}\n",
      "Cosine similarity: 0.9110586716967138\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_TEXT = read_text(\"texto.txt\")\n",
    "SYNTETIC_TEXT = \"texto_sintetico.txt\"\n",
    "\n",
    "#Problem 2\n",
    "markov_model = markovify.Text(ORIGINAL_TEXT) # Build the Markov model\n",
    "synthetic_text = generate_synthetic_text(markov_model, num_sentences=100)\n",
    "\n",
    "save_text(SYNTETIC_TEXT, synthetic_text)\n",
    "\n",
    "#Problem 3\n",
    "original_words = preprocess_text(ORIGINAL_TEXT)\n",
    "synthetic_words = preprocess_text(synthetic_text)\n",
    "idf_dict = compute_idf([original_words, synthetic_words])\n",
    "\n",
    "original_tfidf_vector = transform_to_tfidf_vector(ORIGINAL_TEXT, idf_dict)\n",
    "synthetic_tfidf_vector = transform_to_tfidf_vector(synthetic_text, idf_dict)\n",
    "\n",
    "print(\"Original TF-IDF vector:\\n\", original_tfidf_vector)\n",
    "print(\"Synthetic TF-IDF vector:\\n\", synthetic_tfidf_vector)\n",
    "\n",
    "#Problem 4\n",
    "similarity_score = calculate_cosine_similarity(original_tfidf_vector, synthetic_tfidf_vector)\n",
    "print(\"Cosine similarity:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF vector:\n",
      " {'//': -0.2647391311512829, 'c++': 0.0, 'program': 0.0, \"dijkstra's\": 0.0, 'single': 0.0, 'source': 0.0, 'shortest': 0.0, 'path': 0.0, 'algorithm.': 0.0, 'adjacency': 0.0, 'matrix': 0.0, 'representation': 0.0, 'graph': 0.0, '#include': 0.0, '<iostream>': 0.0, 'using': 0.0, 'namespace': 0.0, 'std;': 0.0, 'number': 0.0, 'vertices': 0.0, '#define': 0.0, 'v': 0.0, '9': 0.0, 'utility': 0.0, 'function': 0.0, 'vertex': 0.0, 'minimum': 0.0, 'distance': 0.0, 'value,': 0.0, 'set': 0.0, 'included': 0.0, 'tree': 0.0, 'int': 0.0, 'mindistance(int': 0.0, 'dist[],': 0.0, 'bool': 0.0, 'sptset[])': 0.0, '{': 0.0, 'initialize': 0.0, 'min': 0.0, 'value': 0.0, '=': 0.0, 'int_max,': 0.0, 'min_index;': 0.0, '(int': 0.0, '0;': 0.0, '<': 0.0, 'v;': 0.0, 'v++)': 0.0, '(sptset[v]': 0.0, '==': 0.0, 'false': 0.0, '&&': 0.0, 'dist[v]': 0.0, '<=': 0.0, 'min)': 0.0, 'dist[v],': 0.0, 'min_index': 0.0, 'return': 0.0, '}': 0.0, 'print': 0.0, 'constructed': 0.0, 'array': 0.0, 'void': 0.0, 'printsolution(int': 0.0, 'dist[])': 0.0, 'cout': 0.0, '<<': 0.0, '\"vertex': 0.0, '\\\\t': 0.0, 'source\"': 0.0, 'endl;': 0.0, 'i++)': 0.0, '\"': 0.0, '\\\\t\\\\t\\\\t\\\\t\"': 0.0, 'dist[i]': 0.0, 'implements': 0.0, 'algorithm': 0.0, 'represented': 0.0, 'dijkstra(int': 0.0, 'graph[v][v],': 0.0, 'src)': 0.0, 'dist[v];': 0.0, 'output': 0.0, 'array.': 0.0, 'hold': 0.0, 'src': 0.0, 'sptset[v];': 0.0, 'sptset[i]': 0.0, 'true': 0.0, 'finalized': 0.0, 'distances': 0.0, 'infinite': 0.0, 'stpset[]': 0.0, 'false;': 0.0, '0': 0.0, 'dist[src]': 0.0, 'count': 0.0, '-': 0.0, '1;': 0.0, 'count++)': 0.0, 'pick': 0.0, 'processed.': 0.0, 'u': 0.0, 'equal': 0.0, 'iteration.': 0.0, 'mindistance(dist,': 0.0, 'sptset);': 0.0, 'mark': 0.0, 'picked': 0.0, 'processed': 0.0, 'sptset[u]': 0.0, 'true;': 0.0, 'update': 0.0, 'dist': 0.0, 'adjacent': 0.0, 'vertex.': 0.0, 'sptset,': 0.0, 'edge': 0.0, 'v,': 0.0, 'total': 0.0, 'weight': 0.0, 'smaller': 0.0, 'current': 0.0, '(!sptset[v]': 0.0, 'graph[u][v]': 0.0, 'dist[u]': 0.0, '!=': 0.0, 'int_max': 0.0, '+': -0.007929060894132211, 'dist[v])': 0.0, 'graph[u][v];': 0.0, 'printsolution(dist);': 0.0, \"driver's\": 0.0, 'code': 0.0, 'main()': 0.0, '/*': 0.0, 'let': 0.0, 'create': 0.0, 'example': 0.0, 'discussed': 0.0, '*/': 0.0, 'graph[v][v]': 0.0, '0,': 0.0, '4,': 0.0, '8,': 0.0, '},': 0.0, '11,': 0.0, '7,': 0.0, '2': 0.0, '9,': 0.0, '14,': 0.0, '10,': 0.0, '2,': 0.0, '1,': 0.0, '6': 0.0, '7': 0.0, '6,': 0.0, '};': 0.0, 'dijkstra(graph,': 0.0, '0);': 0.0, 'contributed': 0.0, 'shivanisinghss2110': 0.0}\n",
      "Synthetic TF-IDF vector:\n",
      " {'//': -0.21932149756468486, 'c++': 0.0, 'program': 0.0, 'implement': 0.0, 'a*': 0.0, 'search': 0.0, 'algorithm': 0.0, '#include': 0.0, '<bits/stdc++.h>': 0.0, 'using': 0.0, 'namespace': 0.0, 'std;': 0.0, '#define': 0.0, 'row': 0.0, '9': 0.0, 'col': 0.0, '10': 0.0, 'creating': 0.0, 'shortcut': 0.0, 'int,': 0.0, 'int': 0.0, 'pair': 0.0, 'type': 0.0, 'typedef': 0.0, 'pair<int,': 0.0, 'int>': 0.0, 'pair;': 0.0, 'int>>': 0.0, 'pair<double,': 0.0, '>': 0.0, 'ppair;': 0.0, 'structure': 0.0, 'hold': 0.0, 'necessary': 0.0, 'parameters': 0.0, 'struct': 0.0, 'cell': 0.0, '{': 0.0, 'column': 0.0, 'index': 0.0, 'parent': 0.0, 'note': 0.0, '0': 0.0, '<=': 0.0, 'row-1': 0.0, '&': 0.0, 'j': 0.0, 'col-1': 0.0, 'parent_i,': 0.0, 'parent_j;': 0.0, 'f': 0.0, '=': 0.0, 'g': 0.0, '+': -0.07729515060482045, 'h': 0.0, 'double': 0.0, 'f,': 0.0, 'g,': 0.0, 'h;': 0.0, '};': 0.0, 'utility': 0.0, 'function': 0.0, 'check': 0.0, 'given': 0.0, '(row,': 0.0, 'col)': 0.0, 'valid': 0.0, 'not.': 0.0, 'bool': 0.0, 'isvalid(int': 0.0, 'row,': 0.0, 'returns': 0.0, 'true': 0.0, 'number': 0.0, 'range': 0.0, 'return': 0.0, '(row': 0.0, '>=': 0.0, '0)': 0.0, '&&': 0.0, '<': 0.0, 'row)': 0.0, '(col': 0.0, 'col);': 0.0, '}': 0.0, 'blocked': 0.0, 'isunblocked(int': 0.0, 'grid[][col],': 0.0, 'false': 0.0, '(grid[row][col]': 0.0, '==': 0.0, '1)': 0.0, '(true);': 0.0, '(false);': 0.0, 'destination': 0.0, 'reached': 0.0, 'isdestination(int': 0.0, 'col,': 0.0, 'dest)': 0.0, 'dest.first': 0.0, 'dest.second)': 0.0, 'calculate': 0.0, \"'h'\": 0.0, 'heuristics.': 0.0, 'calculatehvalue(int': 0.0, 'distance': 0.0, 'formula': 0.0, '((double)sqrt(': 0.0, '-': 0.0, 'dest.first)': 0.0, '*': 0.0, 'dest.second)));': 0.0, 'trace': 0.0, 'path': 0.0, 'source': 0.0, 'void': 0.0, 'tracepath(cell': 0.0, 'celldetails[][col],': 0.0, 'printf(\"\\\\nthe': 0.0, '\");': 0.0, 'dest.first;': 0.0, 'dest.second;': 0.0, 'stack<pair>': 0.0, 'path;': 0.0, '(!(celldetails[row][col].parent_i': 0.0, 'celldetails[row][col].parent_j': 0.0, 'col))': 0.0, 'path.push(make_pair(row,': 0.0, 'col));': 0.0, 'temp_row': 0.0, 'celldetails[row][col].parent_i;': 0.0, 'temp_col': 0.0, 'celldetails[row][col].parent_j;': 0.0, 'temp_row;': 0.0, 'temp_col;': 0.0, '(!path.empty())': 0.0, 'p': 0.0, 'path.top();': 0.0, 'path.pop();': 0.0, 'printf(\"->': 0.0, '(%d,%d)': 0.0, '\",': 0.0, 'p.first,': 0.0, 'p.second);': 0.0, 'return;': 0.0, 'shortest': 0.0, 'according': 0.0, 'astarsearch(int': 0.0, 'src,': 0.0, '(isvalid(src.first,': 0.0, 'src.second)': 0.0, 'false)': 0.0, 'printf(\"source': 0.0, 'invalid\\\\n\");': 0.0, '(isvalid(dest.first,': 0.0, 'printf(\"destination': 0.0, '(isunblocked(grid,': 0.0, 'src.first,': 0.0, '||': 0.0, 'isunblocked(grid,': 0.0, 'dest.first,': 0.0, 'blocked\\\\n\");': 0.0, '(isdestination(src.first,': 0.0, 'src.second,': 0.0, 'true)': 0.0, 'printf(\"we': 0.0, 'destination\\\\n\");': 0.0, 'create': 0.0, 'closed': 0.0, 'list': 0.0, 'initialise': 0.0, 'means': 0.0, 'included': 0.0, 'implemented': 0.0, 'boolean': 0.0, '2d': 0.0, 'array': 0.0, 'closedlist[row][col];': 0.0, 'memset(closedlist,': 0.0, 'false,': 0.0, 'sizeof(closedlist));': 0.0, 'declare': 0.0, 'details': 0.0, 'celldetails[row][col];': 0.0, 'i,': 0.0, 'j;': 0.0, '(i': 0.0, '0;': 0.0, 'row;': 0.0, 'i++)': 0.0, '(j': 0.0, 'col;': 0.0, 'j++)': 0.0, 'celldetails[i][j].f': 0.0, 'flt_max;': 0.0, 'celldetails[i][j].g': 0.0, 'celldetails[i][j].h': 0.0, 'celldetails[i][j].parent_i': 0.0, '-1;': 0.0, 'celldetails[i][j].parent_j': 0.0, 'initialising': 0.0, 'starting': 0.0, 'node': 0.0, 'src.second;': 0.0, '0.0;': 0.0, 'i;': 0.0, '/*': 0.0, 'open': 0.0, 'having': 0.0, 'information': 0.0, 'as-': 0.0, '<f,': 0.0, '<i,': 0.0, 'j>>': 0.0, 'h,': 0.0, 'set': 0.0, 'pair.*/': 0.0, 'set<ppair>': 0.0, 'openlist;': 0.0, \"'f'\": 0.0, 'openlist.insert(make_pair(0.0,': 0.0, 'make_pair(i,': 0.0, 'j)));': 0.0, 'value': 0.0, 'initially': 0.0, 'reached.': 0.0, 'founddest': 0.0, 'false;': 0.0, '(!openlist.empty())': 0.0, 'ppair': 0.0, '*openlist.begin();': 0.0, 'remove': 0.0, 'vertex': 0.0, 'openlist.erase(openlist.begin());': 0.0, 'add': 0.0, 'p.second.first;': 0.0, 'p.second.second;': 0.0, 'closedlist[i][j]': 0.0, 'true;': 0.0, 'generating': 0.0, '8': 0.0, 'successor': 0.0, 'n.w': 0.0, 'n': 0.0, 'n.e': 0.0, '\\\\': 0.0, '|': 0.0, '/': 0.0, 'w----cell----e': 0.0, 's.w': 0.0, 's': 0.0, 's.e': 0.0, 'cell-->popped': 0.0, '(i,': 0.0, 'j)': 0.0, '-->': 0.0, 'north': 0.0, '(i-1,': 0.0, 'south': 0.0, '(i+1,': 0.0, 'e': 0.0, 'east': 0.0, 'j+1)': 0.0, 'w': 0.0, 'west': 0.0, 'j-1)': 0.0, 'n.e-->': 0.0, 'north-east': 0.0, 'n.w-->': 0.0, 'north-west': 0.0, 's.e-->': 0.0, 'south-east': 0.0, 's.w-->': 0.0, 'south-west': 0.0, 'j-1)*/': 0.0, 'store': 0.0, \"'g',\": 0.0, 'successors': 0.0, 'gnew,': 0.0, 'hnew,': 0.0, 'fnew;': 0.0, '//-----------': 0.0, '1st': 0.0, '(north)': 0.0, '------------': 0.0, 'process': 0.0, '(isvalid(i': 0.0, '1,': 0.0, 'current': 0.0, '(isdestination(i': 0.0, 'j,': 0.0, 'celldetails[i': 0.0, '1][j].parent_i': 0.0, '1][j].parent_j': 0.0, 'printf(\"the': 0.0, 'found\\\\n\");': 0.0, 'tracepath(celldetails,': 0.0, 'dest);': 0.0, 'blocked,': 0.0, 'ignore': 0.0, 'it.': 0.0, 'following': 0.0, '(closedlist[i': 0.0, '1][j]': 0.0, 'gnew': 0.0, '1.0;': 0.0, 'hnew': 0.0, 'calculatehvalue(i': 0.0, 'fnew': 0.0, 'hnew;': 0.0, 'isn’t': 0.0, 'list,': 0.0, 'list.': 0.0, 'make': 0.0, 'square': 0.0, 'square.': 0.0, 'record': 0.0, 'costs': 0.0, 'already,': 0.0, 'better,': 0.0, 'cost': 0.0, 'measure.': 0.0, '(celldetails[i': 0.0, '1][j].f': 0.0, 'flt_max': 0.0, 'fnew)': 0.0, 'openlist.insert(make_pair(': 0.0, 'fnew,': 0.0, 'make_pair(i': 0.0, 'update': 0.0, '1][j].g': 0.0, 'gnew;': 0.0, '1][j].h': 0.0, '2nd': 0.0, '(south)': 0.0, '3rd': 0.0, '(east)': 0.0, '(isvalid(i,': 0.0, '(isdestination(i,': 0.0, 'celldetails[i][j': 0.0, '1].parent_i': 0.0, '1].parent_j': 0.0, '(closedlist[i][j': 0.0, '1]': 0.0, 'calculatehvalue(i,': 0.0, '(celldetails[i][j': 0.0, '1].f': 0.0, '1)));': 0.0, '1].g': 0.0, '1].h': 0.0, '4th': 0.0, '(west)': 0.0, '5th': 0.0, '(north-east)': 0.0, '//------------': 0.0, '1][j': 0.0, '1.414;': 0.0, '6th': 0.0, '(north-west)': 0.0, '7th': 0.0, '(south-east)': 0.0, '8th': 0.0, '(south-west)': 0.0, 'empty,': 0.0, 'conclude': 0.0, 'failed': 0.0, 'reach': 0.0, 'cell.': 0.0, 'happen': 0.0, 'way': 0.0, '(due': 0.0, 'blockages)': 0.0, '(founddest': 0.0, 'printf(\"failed': 0.0, 'cell\\\\n\");': 0.0, 'driver': 0.0, 'test': 0.0, 'main()': 0.0, 'description': 0.0, 'grid-': 0.0, '1-->': 0.0, '0-->': 0.0, '*/': 0.0, 'grid[row][col]': 0.0, '0,': 0.0, '1': 0.0, '},': 0.0, 'left-most': 0.0, 'bottom-most': 0.0, 'corner': 0.0, 'src': 0.0, 'make_pair(8,': 0.0, '0);': 0.0, 'top-most': 0.0, 'dest': 0.0, 'make_pair(0,': 0.0, 'astarsearch(grid,': 0.0, '(0);': 0.0}\n",
      "Cosine similarity: 0.9526700316637161\n"
     ]
    }
   ],
   "source": [
    "#Problem 5\n",
    "CODE1 = read_text(\"code1.cpp\")\n",
    "CODE2 = read_text(\"code2.cpp\")\n",
    "\n",
    "preprocess_code1 = preprocess_code(CODE1)\n",
    "preprocess_code2 = preprocess_code(CODE2)\n",
    "idf_dict = compute_idf([preprocess_code1, preprocess_code2])\n",
    "\n",
    "code1_tfidf_vector = transform_to_tfidf_vector(CODE1, idf_dict)\n",
    "code2_tfidf_vector = transform_to_tfidf_vector(CODE2, idf_dict)\n",
    "\n",
    "print(\"Original TF-IDF vector:\\n\", code1_tfidf_vector)\n",
    "print(\"Synthetic TF-IDF vector:\\n\", code2_tfidf_vector)\n",
    "\n",
    "similarity_score_code = calculate_cosine_similarity(code1_tfidf_vector, code2_tfidf_vector)\n",
    "print(\"Cosine similarity:\", similarity_score_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
